{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BAM1D Physics Emulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deniseiras/BAM1D-Microphysics-Emulation/blob/main/BAM1D_Physics_Emulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV16J6oUY-HN"
      },
      "source": [
        "# BAM1D Physics Emulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rwl1iXIKxkm"
      },
      "source": [
        "<font size=\"5\">**Objetivo:** Emular a Hugh Morrison Microphysics implementada em BAM1D</font>\n",
        " \n",
        "\n",
        "\n",
        "SUBROUTINE RunMicro_HugMorr(  \n",
        "  \n",
        "**ncols** - numero de colunas - fixo = 1  \n",
        "nCols       , &!INTEGER      , INTENT(IN   ) :: nCols\n",
        "\n",
        "**kmax** - número de níveis = k_max  \n",
        "kMax        , &!INTEGER      , INTENT(IN   ) :: kMax \n",
        "\n",
        "**si** - kmax+1 removido, sem valores/usado\n",
        "si          , &!REAL(KIND=r8), INTENT(IN   ) :: si(kMax+1)\n",
        "\n",
        "**sl**  \n",
        "sl          , &!REAL(KIND=r8), INTENT(IN   ) :: sl(kMax)\n",
        "\n",
        "**tc** - Temperature(K)  \n",
        "tc          , &!REAL(KIND=r8), INTENT(INOUT) :: Tc (1:nCols, 1:kMax)\n",
        "\n",
        "<font size=\"5\">Q's - Mixing ratio </font>\n",
        "\n",
        "**QV** - water vapor  \n",
        "QV          , &!REAL(KIND=r8), INTENT(INOUT) :: qv (1:nCols, 1:kMax)\n",
        "\n",
        "**QC** - cloud water  \n",
        "QC          , &!REAL(KIND=r8), INTENT(INOUT) :: qc (1:nCols, 1:kMax)\n",
        "\n",
        "**QR**  - rain water  \n",
        "QR          , &!REAL(KIND=r8), INTENT(INOUT) :: qr (1:nCols, 1:kMax)\n",
        "\n",
        "**QI** - cloud ice  \n",
        "QI          , &!REAL(KIND=r8), INTENT(INOUT) :: qi (1:nCols, 1:kMax)\n",
        "\n",
        "**QS** - Snow  \n",
        "QS          , &!REAL(KIND=r8), INTENT(INOUT) :: qs (1:nCols, 1:kMax)\n",
        "\n",
        "**QG** - Graupel  \n",
        "QG          , &!REAL(KIND=r8), INTENT(INOUT) :: qg (1:nCols, 1:kMax)\n",
        "\n",
        "<font size=\"5\">*N's* - number concentration\n",
        "</font>  \n",
        "\n",
        "**NI** - cloud ice  \n",
        "NI          , &!REAL(KIND=r8), INTENT(INOUT) :: ni (1:nCols, 1:kMax)\n",
        "\n",
        "**NS** - Snow  \n",
        "NS          , &!REAL(KIND=r8), INTENT(INOUT) :: ns (1:nCols, 1:kMax)\n",
        "\n",
        "**NR** - Rain    \n",
        "NR          , &!REAL(KIND=r8), INTENT(INOUT) :: nr (1:nCols, 1:kMax)\n",
        "\n",
        "**NG** - Graupel  \n",
        "NG          , &!REAL(KIND=r8), INTENT(INOUT) :: NG (1:nCols, 1:kMax)   \n",
        "\n",
        "**NC** - Cloud droplet  \n",
        "NC          , &!REAL(KIND=r8), INTENT(INOUT) :: NC (1:nCols, 1:kMax)   \n",
        "\n",
        "**TKE** - turbulence kinetic energy (m^2 s-2), NEEDED FOR DROPLET ACTIVATION  \n",
        "TKE         , &!REAL(KIND=r8), INTENT(IN   ) :: TKE (1:nCols, 1:kMax)   \n",
        "\n",
        "**KZH** -  heat eddy diffusion coefficient from YSU scheme (M^2 S-1), NEEDED FOR DROPLET ACTIVATION  \n",
        "KZH         , &!REAL(KIND=r8), INTENT(IN   ) :: KZH (1:nCols, 1:kMax)   \n",
        "\n",
        "**gps** - Pressão - removida da entrada da RNA - valor fixo  \n",
        "gps         , &!gps- AIR PRESSURE (PA)\n",
        "\n",
        "**DT_IN** - model time step (sec) - Removido da entrada da RNA\n",
        "DT_IN       , &!REAL(KIND=r8), INTENT(IN   ) :: dt_in\n",
        "\n",
        "**omega**  \n",
        "omega       , &!REAL(KIND=r8), INTENT(IN   ) :: omega  ! omega (Pa/s)\n",
        "\n",
        "**EFFCS** - CLOUD DROPLET EFFECTIVE RADIUS OUTPUT TO RADIATION CODE (micron)  \n",
        "EFFCS       , &!REAL(KIND=r8), INTENT(OUT  ) :: EFFCS (1:nCols, 1:kMax)   \n",
        "\n",
        "**EFFIS** - ICE DROPLET EFFECTIVE RADIUS OUTPUT TO RADIATION CODE (micron)  \n",
        "EFFIS       , &!REAL(KIND=r8), INTENT(OUT  ) :: EFFIS (1:nCols, 1:kMax)   \n",
        "\n",
        "**LSRAIN** - Large scale Rain  \n",
        "LSRAIN      , &!REAL(KIND=r8), INTENT(OUT) :: LSRAIN(1:nCols)\n",
        "\n",
        "**LSSNOW** - Large scale Snow  \n",
        "LSSNOW        )!REAL(KIND=r8), INTENT(OUT) :: LSSNOW(1:nCols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBPTONWzKxkn"
      },
      "source": [
        "# Setup do dados\n",
        "\n",
        "Carregamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtYVuONUKxko",
        "outputId": "ead05257-c80f-4031-fe02-db259648ac7f"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from sklearn import metrics, preprocessing\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "import keras\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Input, Dropout, Conv1D, Flatten, MaxPooling1D, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "import csv    \n",
        "\n",
        "from datetime import datetime\n",
        "from shutil import copyfile, copytree, rmtree\n",
        "\n",
        "k_max = 28  # número de níveis utilizado\n",
        "\n",
        "# % pip install XlsxWriter\n",
        "# % pip install SkillMetrics\n",
        "# import skill_metrics as sm\n",
        "\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "# pd.options.display.float_format = '{:.1f}'.format\n",
        "pd.options.display.float_format = '{:f}'.format\n",
        "\n",
        "\n",
        "# working local\n",
        "#\n",
        "# %load_ext tensorboard funciona local ???\n",
        "# %reload_ext tensorboard ???\n",
        "# colab_in_drive_root_dir = \"/home/denis/_COM_BACKUP/NN_BAM1d/bam1d_data/Colab Notebooks\"\n",
        "# tb_logdir_base = colab_in_drive_root_dir + '/logs'\n",
        "# data_in_drive_dir = \"/media/denis/dados/_COM_BACKUP/NN_BAM1D/bam1d_data/\"\n",
        "\n",
        "\n",
        "# loading from gdrive - COLAB MOUNTS AUTOMATICALLY NOW\n",
        "#\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "tb_logdir_base = './logs'\n",
        "colab_in_drive_root_dir = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "data_in_drive_dir = \"/content/drive/My Drive/NN_Microphysics/\"\n",
        "\n",
        "# original_input_df = pd.read_csv(\"{}/hug_morr_inputs_CRD_RRTMG.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "# original_output_df = pd.read_csv(\"{}/hug_morr_outputs_CRD_RRTMG.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n",
        "# original_input_df = pd.read_csv(\"{}/hug_morr_inputs_CRD_HSR.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "# original_output_df = pd.read_csv(\"{}/hug_morr_outputs_CRD_HSR.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n",
        "# original_input_df = pd.read_csv(\"{}/hug_morr_inputs_CRD_RRTMG___dt_60.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "# original_output_df = pd.read_csv(\"{}/hug_morr_outputs_CRD_RRTMG___dt_60.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n",
        "\n",
        "# IOP com menos de 180 falta memória\n",
        "\n",
        "# original_input_df = pd.read_csv(\"{}/IOP2014__CRD_RRTMG_dt_360/hug_morr_inputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "# original_output_df = pd.read_csv(\"{}/IOP2014__CRD_RRTMG_dt_360/hug_morr_outputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n",
        "# original_input_df = pd.read_csv(\"{}IOP2014__CRD_RRMTG_dt_60/hug_morr_inputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "# original_output_df = pd.read_csv(\"{}IOP2014__CRD_RRMTG_dt_60/hug_morr_outputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n",
        "# original_input_df = pd.read_csv(\"{}IOP2014__CRD_RRMTG_dt_120/hug_morr_inputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "# original_output_df = pd.read_csv(\"{}IOP2014__CRD_RRMTG_dt_120/hug_morr_outputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n",
        "original_input_df = pd.read_csv(\"{}IOP2014__CRD_RRMTG_dt_180/hug_morr_inputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "original_output_df = pd.read_csv(\"{}IOP2014__CRD_RRMTG_dt_180/hug_morr_outputs.csv\".format(data_in_drive_dir), sep=\",\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjrd-l4Fcfdo"
      },
      "source": [
        "## Análise dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgxQXkVeqWTc"
      },
      "source": [
        "### Análise estatística\n",
        "\n",
        "- Resumo dos valores das variáveis de entrada e saída. Observe que os valores são truncados no Resumo\n",
        "- Valores do primeiro, centésimo e último passo de tempo\n",
        "- Níveis verticais do primeiro, centésimo e último passo de tempo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah6LjMIJ2spZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "301ef044-3fce-4a8c-96c5-19c15e12db2a"
      },
      "source": [
        "print(\"\\nAll input summary:\")\n",
        "display.display(original_input_df.describe())\n",
        "print(\"\\nAll output summary:\")\n",
        "display.display(original_output_df.describe())\n",
        "\n",
        "print(\"\\nInput First timestep:\")\n",
        "display.display(original_input_df.head(k_max))\n",
        "print(\"\\nInput 100th timestep:\")\n",
        "display.display(original_input_df.loc[k_max*100:k_max*100+27])\n",
        "print(\"\\nInput Last timestep:\")\n",
        "display.display(original_input_df.tail(k_max))\n",
        "\n",
        "print(\"\\nOutput First timestep:\")\n",
        "display.display(original_output_df.head(k_max))\n",
        "print(\"\\nOutput 100th timestep:\")\n",
        "display.display(original_output_df.loc[k_max*100:k_max*100+27])\n",
        "print(\"\\nOutput Last timestep:\")\n",
        "display.display(original_output_df.tail(k_max))\n",
        "\n",
        "# original_output_df.describe().to_csv('/content/drive/My Drive/Colab Notebooks/temp_describe.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "All input summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>si</th>\n",
              "      <th>si_kmax_+_1</th>\n",
              "      <th>sl</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>tke</th>\n",
              "      <th>kzh</th>\n",
              "      <th>gps</th>\n",
              "      <th>omega</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>0.468551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450519</td>\n",
              "      <td>237.808650</td>\n",
              "      <td>0.005162</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2358.732837</td>\n",
              "      <td>110745.661609</td>\n",
              "      <td>3917.264959</td>\n",
              "      <td>0.046469</td>\n",
              "      <td>97568.358504</td>\n",
              "      <td>0.469912</td>\n",
              "      <td>47.568224</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.011695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.077748</td>\n",
              "      <td>0.363561</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.361058</td>\n",
              "      <td>61.352569</td>\n",
              "      <td>0.005985</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>34806.115287</td>\n",
              "      <td>403911.484787</td>\n",
              "      <td>46250.692154</td>\n",
              "      <td>6.470318</td>\n",
              "      <td>857150.326910</td>\n",
              "      <td>1.438855</td>\n",
              "      <td>96.230471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>63.503048</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-1.076957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.750000</td>\n",
              "      <td>0.103880</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090635</td>\n",
              "      <td>202.558529</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.023818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>0.432240</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.399916</td>\n",
              "      <td>257.458090</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.250000</td>\n",
              "      <td>0.834470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.811268</td>\n",
              "      <td>288.992577</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30.092852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>32.316904</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.015078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994964</td>\n",
              "      <td>308.962369</td>\n",
              "      <td>0.016233</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>1145094.435400</td>\n",
              "      <td>2942266.541640</td>\n",
              "      <td>1521556.569890</td>\n",
              "      <td>6038.505010</td>\n",
              "      <td>35494575.117600</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.541837</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   k             si  ...            gps          omega\n",
              "count 4865308.000000 4865308.000000  ... 4865308.000000 4865308.000000\n",
              "mean       14.500000       0.468551  ...   99000.000000      -0.011695\n",
              "std         8.077748       0.363561  ...       0.000000       0.079032\n",
              "min         1.000000       0.003669  ...   99000.000000      -1.076957\n",
              "25%         7.750000       0.103880  ...   99000.000000      -0.023818\n",
              "50%        14.500000       0.432240  ...   99000.000000       0.000000\n",
              "75%        21.250000       0.834470  ...   99000.000000       0.015078\n",
              "max        28.000000       1.000000  ...   99000.000000       0.541837\n",
              "\n",
              "[8 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "All output summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>EFFCS</th>\n",
              "      <th>EFFIS</th>\n",
              "      <th>LSRAIN</th>\n",
              "      <th>LSSNOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "      <td>4865308.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>237.808947</td>\n",
              "      <td>0.005162</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2358.846785</td>\n",
              "      <td>110746.708095</td>\n",
              "      <td>3917.269937</td>\n",
              "      <td>0.046469</td>\n",
              "      <td>97568.358504</td>\n",
              "      <td>24.701405</td>\n",
              "      <td>27.424802</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.077748</td>\n",
              "      <td>61.351244</td>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>34807.013005</td>\n",
              "      <td>403913.164816</td>\n",
              "      <td>46250.692216</td>\n",
              "      <td>6.470318</td>\n",
              "      <td>857150.326910</td>\n",
              "      <td>2.230185</td>\n",
              "      <td>14.292927</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.503048</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.750000</td>\n",
              "      <td>202.558915</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>257.522230</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.250000</td>\n",
              "      <td>288.991030</td>\n",
              "      <td>0.009955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30.095493</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>308.962369</td>\n",
              "      <td>0.016276</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>1145094.435400</td>\n",
              "      <td>2942266.541640</td>\n",
              "      <td>1521556.569890</td>\n",
              "      <td>6038.505010</td>\n",
              "      <td>35494575.117600</td>\n",
              "      <td>27.581993</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   k             Tc  ...         LSRAIN         LSSNOW\n",
              "count 4865308.000000 4865308.000000  ... 4865308.000000 4865308.000000\n",
              "mean       14.500000     237.808947  ...       0.000001       0.000000\n",
              "std         8.077748      61.351244  ...       0.000004       0.000000\n",
              "min         1.000000      63.503048  ...      -0.000000      -0.000000\n",
              "25%         7.750000     202.558915  ...       0.000000       0.000000\n",
              "50%        14.500000     257.522230  ...       0.000000       0.000000\n",
              "75%        21.250000     288.991030  ...       0.000000       0.000000\n",
              "max        28.000000     308.962369  ...       0.000163       0.000000\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input First timestep:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>si</th>\n",
              "      <th>si_kmax_+_1</th>\n",
              "      <th>sl</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>tke</th>\n",
              "      <th>kzh</th>\n",
              "      <th>gps</th>\n",
              "      <th>omega</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994964</td>\n",
              "      <td>300.672944</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060765</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.012614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.989934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.981964</td>\n",
              "      <td>300.526379</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.143061</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.001955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.974009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.964137</td>\n",
              "      <td>299.960846</td>\n",
              "      <td>0.015145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.028920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.954290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.942167</td>\n",
              "      <td>298.651604</td>\n",
              "      <td>0.014704</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.037237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.930081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.915361</td>\n",
              "      <td>296.805719</td>\n",
              "      <td>0.014252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.033169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>0.042912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035429</td>\n",
              "      <td>201.415438</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>0.028306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>207.610504</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011763</td>\n",
              "      <td>211.525561</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005429</td>\n",
              "      <td>213.996873</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>215.543717</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     k       si  si_kmax_+_1       sl  ...      tke      kzh          gps     omega\n",
              "0    1 1.000000     0.000000 0.994964  ... 0.000000 0.060765 99000.000000 -0.012614\n",
              "1    2 0.989934     0.000000 0.981964  ... 0.000000 0.143061 99000.000000 -0.001955\n",
              "2    3 0.974009     0.000000 0.964137  ... 0.000000 0.100000 99000.000000 -0.028920\n",
              "3    4 0.954290     0.000000 0.942167  ... 0.000000 0.100000 99000.000000 -0.037237\n",
              "4    5 0.930081     0.000000 0.915361  ... 0.000000 0.100000 99000.000000 -0.033169\n",
              "..  ..      ...          ...      ...  ...      ...      ...          ...       ...\n",
              "23  24 0.042912     0.000000 0.035429  ... 0.000000 0.100000 99000.000000  0.000000\n",
              "24  25 0.028306     0.000000 0.022285  ... 0.000000 0.100000 99000.000000  0.000000\n",
              "25  26 0.016629     0.000000 0.011763  ... 0.000000 0.100000 99000.000000  0.000000\n",
              "26  27 0.007338     0.000000 0.005429  ... 0.000000 0.100000 99000.000000  0.000000\n",
              "27  28 0.003669     0.000000 0.001523  ... 0.000000 0.100000 99000.000000  0.000000\n",
              "\n",
              "[28 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input 100th timestep:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>si</th>\n",
              "      <th>si_kmax_+_1</th>\n",
              "      <th>sl</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>tke</th>\n",
              "      <th>kzh</th>\n",
              "      <th>gps</th>\n",
              "      <th>omega</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2800</th>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994964</td>\n",
              "      <td>302.660529</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.711810</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.001430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2801</th>\n",
              "      <td>2</td>\n",
              "      <td>0.989934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.981964</td>\n",
              "      <td>301.403797</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200121</td>\n",
              "      <td>15.619333</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.002425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2802</th>\n",
              "      <td>3</td>\n",
              "      <td>0.974009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.964137</td>\n",
              "      <td>299.771242</td>\n",
              "      <td>0.015145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.196657</td>\n",
              "      <td>23.227907</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>-0.000621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2803</th>\n",
              "      <td>4</td>\n",
              "      <td>0.954290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.942167</td>\n",
              "      <td>297.668440</td>\n",
              "      <td>0.014704</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.237860</td>\n",
              "      <td>18.964212</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.004144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2804</th>\n",
              "      <td>5</td>\n",
              "      <td>0.930081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.915361</td>\n",
              "      <td>295.257251</td>\n",
              "      <td>0.014252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082481</td>\n",
              "      <td>8.552701</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.010388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2823</th>\n",
              "      <td>24</td>\n",
              "      <td>0.042912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035429</td>\n",
              "      <td>199.198687</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824</th>\n",
              "      <td>25</td>\n",
              "      <td>0.028306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>204.941411</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>26</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011763</td>\n",
              "      <td>206.179766</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>27</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005429</td>\n",
              "      <td>208.382146</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2827</th>\n",
              "      <td>28</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>210.473033</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       k       si  si_kmax_+_1  ...       kzh          gps     omega\n",
              "2800   1 1.000000     0.000000  ...  9.711810 99000.000000 -0.001430\n",
              "2801   2 0.989934     0.000000  ... 15.619333 99000.000000 -0.002425\n",
              "2802   3 0.974009     0.000000  ... 23.227907 99000.000000 -0.000621\n",
              "2803   4 0.954290     0.000000  ... 18.964212 99000.000000  0.004144\n",
              "2804   5 0.930081     0.000000  ...  8.552701 99000.000000  0.010388\n",
              "...   ..      ...          ...  ...       ...          ...       ...\n",
              "2823  24 0.042912     0.000000  ...  0.100000 99000.000000  0.000000\n",
              "2824  25 0.028306     0.000000  ...  0.100000 99000.000000  0.000000\n",
              "2825  26 0.016629     0.000000  ...  0.100000 99000.000000  0.000000\n",
              "2826  27 0.007338     0.000000  ...  0.100000 99000.000000  0.000000\n",
              "2827  28 0.003669     0.000000  ...  0.100000 99000.000000  0.000000\n",
              "\n",
              "[28 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input Last timestep:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>si</th>\n",
              "      <th>si_kmax_+_1</th>\n",
              "      <th>sl</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>tke</th>\n",
              "      <th>kzh</th>\n",
              "      <th>gps</th>\n",
              "      <th>omega</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4865280</th>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994964</td>\n",
              "      <td>302.670632</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.841763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.103186</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865281</th>\n",
              "      <td>2</td>\n",
              "      <td>0.989934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.981964</td>\n",
              "      <td>301.725666</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.777762</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>44.312742</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.004870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865282</th>\n",
              "      <td>3</td>\n",
              "      <td>0.974009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.964137</td>\n",
              "      <td>300.275980</td>\n",
              "      <td>0.015145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.588282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.011449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865283</th>\n",
              "      <td>4</td>\n",
              "      <td>0.954290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.942167</td>\n",
              "      <td>298.403754</td>\n",
              "      <td>0.014704</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.097018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.017780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865284</th>\n",
              "      <td>5</td>\n",
              "      <td>0.930081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.915361</td>\n",
              "      <td>296.113026</td>\n",
              "      <td>0.014252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.900801</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.022871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865303</th>\n",
              "      <td>24</td>\n",
              "      <td>0.042912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035429</td>\n",
              "      <td>180.170778</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865304</th>\n",
              "      <td>25</td>\n",
              "      <td>0.028306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>159.627927</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865305</th>\n",
              "      <td>26</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011763</td>\n",
              "      <td>133.086872</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865306</th>\n",
              "      <td>27</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005429</td>\n",
              "      <td>110.417731</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865307</th>\n",
              "      <td>28</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>75.627278</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>99000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          k       si  si_kmax_+_1  ...        kzh          gps    omega\n",
              "4865280   1 1.000000     0.000000  ...  11.103186 99000.000000 0.000416\n",
              "4865281   2 0.989934     0.000000  ...  44.312742 99000.000000 0.004870\n",
              "4865282   3 0.974009     0.000000  ...   0.100000 99000.000000 0.011449\n",
              "4865283   4 0.954290     0.000000  ...   0.100000 99000.000000 0.017780\n",
              "4865284   5 0.930081     0.000000  ...   0.100000 99000.000000 0.022871\n",
              "...      ..      ...          ...  ...        ...          ...      ...\n",
              "4865303  24 0.042912     0.000000  ...   0.100000 99000.000000 0.000000\n",
              "4865304  25 0.028306     0.000000  ... 300.000000 99000.000000 0.000000\n",
              "4865305  26 0.016629     0.000000  ... 300.000000 99000.000000 0.000000\n",
              "4865306  27 0.007338     0.000000  ... 300.000000 99000.000000 0.000000\n",
              "4865307  28 0.003669     0.000000  ... 300.000000 99000.000000 0.000000\n",
              "\n",
              "[28 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output First timestep:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>EFFCS</th>\n",
              "      <th>EFFIS</th>\n",
              "      <th>LSRAIN</th>\n",
              "      <th>LSSNOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>300.672944</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>300.526379</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>299.960846</td>\n",
              "      <td>0.015145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>298.651604</td>\n",
              "      <td>0.014704</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>296.805719</td>\n",
              "      <td>0.014252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>201.415438</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>207.610504</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>211.525561</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>213.996873</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>215.543717</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     k         Tc       qv       qc  ...    EFFCS     EFFIS   LSRAIN   LSSNOW\n",
              "0    1 300.672944 0.016230 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "1    2 300.526379 0.015747 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "2    3 299.960846 0.015145 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "3    4 298.651604 0.014704 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "4    5 296.805719 0.014252 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "..  ..        ...      ...      ...  ...      ...       ...      ...      ...\n",
              "23  24 201.415438 0.000003 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "24  25 207.610504 0.000004 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "25  26 211.525561 0.000014 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "26  27 213.996873 0.000029 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "27  28 215.543717 0.000045 0.000000  ... 1.000000 13.000000 0.000000 0.000000\n",
              "\n",
              "[28 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output 100th timestep:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>EFFCS</th>\n",
              "      <th>EFFIS</th>\n",
              "      <th>LSRAIN</th>\n",
              "      <th>LSSNOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2800</th>\n",
              "      <td>1</td>\n",
              "      <td>302.660529</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2801</th>\n",
              "      <td>2</td>\n",
              "      <td>301.403797</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2802</th>\n",
              "      <td>3</td>\n",
              "      <td>299.771242</td>\n",
              "      <td>0.015145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2803</th>\n",
              "      <td>4</td>\n",
              "      <td>297.668440</td>\n",
              "      <td>0.014704</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2804</th>\n",
              "      <td>5</td>\n",
              "      <td>295.257251</td>\n",
              "      <td>0.014252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2823</th>\n",
              "      <td>24</td>\n",
              "      <td>199.198687</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824</th>\n",
              "      <td>25</td>\n",
              "      <td>204.941411</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>26</td>\n",
              "      <td>206.179766</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>27</td>\n",
              "      <td>208.382146</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2827</th>\n",
              "      <td>28</td>\n",
              "      <td>210.473033</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       k         Tc       qv       qc  ...     EFFCS     EFFIS   LSRAIN   LSSNOW\n",
              "2800   1 302.660529 0.016230 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2801   2 301.403797 0.015747 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2802   3 299.771242 0.015145 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2803   4 297.668440 0.014704 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2804   5 295.257251 0.014252 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "...   ..        ...      ...      ...  ...       ...       ...      ...      ...\n",
              "2823  24 199.198687 0.000003 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2824  25 204.941411 0.000004 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2825  26 206.179766 0.000014 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2826  27 208.382146 0.000029 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "2827  28 210.473033 0.000045 0.000000  ... 25.000000 25.000000 0.000000 0.000000\n",
              "\n",
              "[28 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output Last timestep:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>EFFCS</th>\n",
              "      <th>EFFIS</th>\n",
              "      <th>LSRAIN</th>\n",
              "      <th>LSSNOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4865280</th>\n",
              "      <td>1</td>\n",
              "      <td>302.667146</td>\n",
              "      <td>0.016231</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.530966</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865281</th>\n",
              "      <td>2</td>\n",
              "      <td>301.721692</td>\n",
              "      <td>0.015749</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.721369</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865282</th>\n",
              "      <td>3</td>\n",
              "      <td>300.271134</td>\n",
              "      <td>0.015147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.247833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865283</th>\n",
              "      <td>4</td>\n",
              "      <td>298.397783</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.630182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865284</th>\n",
              "      <td>5</td>\n",
              "      <td>296.105611</td>\n",
              "      <td>0.014255</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>51.484827</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865303</th>\n",
              "      <td>24</td>\n",
              "      <td>180.170778</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865304</th>\n",
              "      <td>25</td>\n",
              "      <td>159.627927</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865305</th>\n",
              "      <td>26</td>\n",
              "      <td>133.086872</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865306</th>\n",
              "      <td>27</td>\n",
              "      <td>110.417731</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4865307</th>\n",
              "      <td>28</td>\n",
              "      <td>75.627278</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          k         Tc       qv       qc  ...     EFFCS     EFFIS   LSRAIN   LSSNOW\n",
              "4865280   1 302.667146 0.016231 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865281   2 301.721692 0.015749 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865282   3 300.271134 0.015147 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865283   4 298.397783 0.014706 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865284   5 296.105611 0.014255 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "...      ..        ...      ...      ...  ...       ...       ...      ...      ...\n",
              "4865303  24 180.170778 0.000003 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865304  25 159.627927 0.000004 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865305  26 133.086872 0.000014 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865306  27 110.417731 0.000029 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "4865307  28  75.627278 0.000045 0.000000  ... 25.000000 25.000000 0.000003 0.000000\n",
              "\n",
              "[28 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDyXQ7hmYl8Y"
      },
      "source": [
        "### Histogramas das variáveis de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLxIh-_UtGLZ"
      },
      "source": [
        "# Input Histograms\n",
        "\n",
        "# _ = original_input_df.hist(bins=50, figsize=(30, 20), xlabelsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQYQYF9YqyZ"
      },
      "source": [
        "### Histogramas das variáveis de saída"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UidFmUfVtGLd"
      },
      "source": [
        "# Output Histograms\n",
        "\n",
        "# _ = original_output_df.hist(bins=50, figsize=(30, 20), xlabelsize=10)\n",
        "#_ = validation_examples.hist(bins=20, figsize=(18, 12), xlabelsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFtL1y3EYba6"
      },
      "source": [
        "### Bloxpot das variáveis de entrada por níveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyjK4MIZYhE9"
      },
      "source": [
        "# Boxplot of variables through levels\n",
        "\n",
        "# for key in original_input_df.columns.tolist():\n",
        "#     if key == \"k\": continue\n",
        "#     # plt.subplot(6, 2, idx_plot)\n",
        "#     original_input_df.boxplot(column=key, by=\"k\", figsize=(15, 6))\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8pki7ZkCOOZ"
      },
      "source": [
        "### Boxplot das variáveis de saída, por níveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3u1YTcYCPFw"
      },
      "source": [
        "# Boxplot of variables through levels\n",
        "\n",
        "# for key in original_output_df.columns.tolist():\n",
        "#     if key == \"k\": continue\n",
        "#     # plt.subplot(6, 2, idx_plot)\n",
        "#     original_output_df.boxplot(column=key, by=\"k\", figsize=(15, 6))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo690q97Y1Cb"
      },
      "source": [
        "### Gráfico de perfil vertical das variáves de entrada em 3 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjLQ39SAtGLn"
      },
      "source": [
        "# Plot the vertical levels of first, 100th and last timestep\n",
        "\n",
        "# first_timestep_input_df = original_input_df.head(k_max)\n",
        "# med_timestep_input_df = original_input_df.loc[k_max00:k_max00+27]\n",
        "# last_timestep_input_df = original_input_df.tail(k_max)\n",
        "\n",
        "# idx_plot = 1\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for key in last_timestep_input_df.columns.tolist():\n",
        "#     plt.subplot(7, 3, idx_plot)\n",
        "#     plt.ylabel(\"k\")\n",
        "#     plt.xlabel(key)\n",
        "#     plt.plot(first_timestep_input_df[key], first_timestep_input_df[[\"k\"]], label=\"first timestep\")\n",
        "#     plt.plot(med_timestep_input_df[key], med_timestep_input_df[[\"k\"]], label=\"100th timestep\")\n",
        "#     plt.plot(last_timestep_input_df[key], last_timestep_input_df[[\"k\"]], label=\"last timestep\")\n",
        "#     idx_plot += 1\n",
        "# plt.legend(loc=\"upper left\")\n",
        "# plt.subplots_adjust(hspace=1, wspace=1)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# first_timestep_output_df = original_output_df.head(k_max)\n",
        "# med_timestep_output_df = original_output_df.loc[k_max01:k_max01+27]\n",
        "# last_timestep_output_df = original_output_df.tail(k_max)\n",
        "\n",
        "# idx_plot = 1\n",
        "# for key in last_timestep_output_df.columns.tolist():\n",
        "#     plt.subplot(4, 6, idx_plot)\n",
        "#     plt.ylabel(\"k\")\n",
        "#     plt.xlabel(key)\n",
        "#     plt.title(\"Output x Level\")\n",
        "#     plt.plot(first_timestep_output_df[key], first_timestep_output_df[[\"k\"]], label=\"first timestep\")\n",
        "#     plt.plot(med_timestep_output_df[key], med_timestep_output_df[[\"k\"]], label=\"100th timestep\")\n",
        "#     plt.plot(last_timestep_output_df[key], last_timestep_output_df[[\"k\"]], label=\"last timestep\")\n",
        "#     plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Www4082tbcDR"
      },
      "source": [
        "## Pré-processando a entrada (exemplos) e saídas (metas)\n",
        "- Selecionamos as primeiras variáveis de entrada e saída que serão usadas no treinamento e na validação;\n",
        "- também podemos criar novas variáveis como um composto de variáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8qC-jTIKxkr"
      },
      "source": [
        "def preprocess_features(input_df):\n",
        "  \"\"\"Prepares input features input_df\n",
        "\n",
        "  Args:\n",
        "    input data frame: A Pandas DataFrame expected to contain data from input data set.\n",
        "  Returns:\n",
        "    A DataFrame that contains the features to be used for the model, including\n",
        "    synthetic features.\n",
        "  \"\"\"\n",
        "# All input variables\n",
        "# \"k\", \"si\", \"si_kmax_+_1\", \"sl\", \"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"tke\", \"kzh\", \"gps\", \"omega\"    \n",
        "  selected_features = input_df[\n",
        "    [\n",
        "    \"k\", \"si\", \"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"tke\", \"kzh\", \"omega\"    \n",
        "    ]]\n",
        "  processed_features = selected_features.copy()\n",
        "  return processed_features\n",
        "\n",
        "\n",
        "def preprocess_targets(output_df):\n",
        "  \"\"\"Prepares target features (i.e., labels) from output_df\n",
        "\n",
        "  Args:\n",
        "    output_df: A Pandas DataFrame expected to contain data from output data set.\n",
        "  Returns:\n",
        "    A DataFrame that contains the target features.\n",
        "  \"\"\"\n",
        "# All output variables\n",
        "# \"k\", \"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"EFFCS\", \"EFFIS\", \"LSRAIN\", \"LSSNOW\"\n",
        "  output_targets = output_df [\n",
        "      [\n",
        "      \"k\", \"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"EFFCS\", \"EFFIS\", \"LSRAIN\", \"LSSNOW\"\n",
        "      ]\n",
        "  ]\n",
        "  return output_targets\n",
        "\n",
        "\n",
        "# execute the pre processing\n",
        "all_examples = preprocess_features(original_input_df)\n",
        "all_targets = preprocess_targets(original_output_df)\n",
        "\n",
        "del original_input_df\n",
        "del original_output_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmLPIHiOip4"
      },
      "source": [
        "### Funções de normalização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxvfgSl2hnlY"
      },
      "source": [
        "def linear_scale(serie_or_np_arr, min_val, max_val):\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "  #   X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "  # X_scaled = X_std * (max - min) + min  \n",
        "  min, max = 0, 1\n",
        "  y = lambda x:( ( x - min_val) / (max_val - min_val)) * (max - min) + min \n",
        "  return y(serie_or_np_arr)\n",
        "  # Linear normalization (serie or numpy 1D array)\n",
        "  # -1 e 1\n",
        "\n",
        "def delinear_scale(serie_or_np_arr, min_val, max_val):\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "  #   X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "  # X_scaled = X_std * (max - min) + min  \n",
        "  min, max = 0, 1\n",
        "  y = lambda x: min_val + (max_val-min_val)*(x - min) / (max - min)      \n",
        "  return y(serie_or_np_arr)\n",
        "\n",
        "\n",
        "def linear_scale_htang(serie_or_np_arr, min_val, max_val):\n",
        "  scale = (max_val - min_val) / 2.0\n",
        "  y = lambda x:((x - min_val) / scale) - 1.0\n",
        "  return y(serie_or_np_arr)\n",
        "\n",
        "# old bak\n",
        "# def linear_scale(serie_or_np_arr):\n",
        "#   min_val = serie_or_np_arr.min()\n",
        "#   max_val = serie_or_np_arr.max()\n",
        "#   scale = (max_val - min_val) / 2.0\n",
        "#   y = lambda x:((x - min_val) / scale) - 1.0\n",
        "#   return y(serie_or_np_arr), min_val, max_val\n",
        "\n",
        "\n",
        "# Linear denormalization (serie or numpy 1D array)\n",
        "#\n",
        "# enter with min and max val of values before normalization\n",
        "def delinear_scale_htang(serie_or_np_arr, min_val, max_val):\n",
        "  scale = (max_val - min_val) / 2.0\n",
        "  y = lambda x:((x + 1.0) * scale) + min_val\n",
        "  return y(serie_or_np_arr)\n",
        "\n",
        "\n",
        "# Different normalilzation functions\n",
        "\n",
        "def log_normalize(series):\n",
        "  return series.apply(lambda x:math.log(x+1.0))\n",
        "\n",
        "def clip(series, clip_to_min, clip_to_max):\n",
        "  return series.apply(lambda x:(\n",
        "    min(max(x, clip_to_min), clip_to_max)))\n",
        "\n",
        "def z_score_normalize(series):\n",
        "  mean = series.mean()\n",
        "  std_dv = series.std()\n",
        "  return series.apply(lambda x:(x - mean) / std_dv)\n",
        "\n",
        "def binary_threshold(series, threshold):\n",
        "  return series.apply(lambda x:(1 if x > threshold else 0))\n",
        "\n",
        "\n",
        "# cols = ['A', 'B']\n",
        "# data =  pd.DataFrame(np.array([[2,3],[1.02,1.2],[0.5,0.3]]),columns=cols)\n",
        "# serie = [2, 1.02, 0.5]\n",
        "# display.display(data)\n",
        "\n",
        "# scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
        "# scaled_data = scaler.fit_transform(data[cols])\n",
        "# display.display(scaled_data)\n",
        "\n",
        "# scaled_data = linear_scale(data, 0.5, 2.0)\n",
        "# display.display(scaled_data)\n",
        "\n",
        "# de_scaled_data = delinear_scale(scaled_data, 0.5, 2.0)\n",
        "# display.display(de_scaled_data)\n",
        "\n",
        "# inv = scaler.inverse_transform(scaled_data)\n",
        "# display.display(inv)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVmuHI76N2Sz"
      },
      "source": [
        "### Normalização da base usando escala linear\n",
        "\n",
        "Normalização das entradas para a escala -1, 1. para os primeiros testes\n",
        "\n",
        "Como regra geral, o NN treina melhor quando os recursos de entrada estão aproximadamente na mesma escala.\n",
        "\n",
        "Isso ajuda o NN a não ficar preso em etapas que são muito grandes em uma dimensão ou muito pequenas em outra.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-bJBXrJx-U_"
      },
      "source": [
        "def normalize_linear_scale(examples_dataframe, minmax_dict=None, scaler=\"normalize\"):\n",
        "  \"\"\"Returns a version of the input `DataFrame` that has all its features normalized linearly.\"\"\"\n",
        "\n",
        "  # Convert pandas data into a dict of np arrays.\n",
        "  processed_features = examples_dataframe.copy()\n",
        "\n",
        "  for key,value in dict(examples_dataframe).items():\n",
        "    if key != 'k':\n",
        "      if minmax_dict is None:\n",
        "        min_val = value.min()\n",
        "        max_val = value.max()\n",
        "      else:\n",
        "        min_val = minmax_dict[key][0]\n",
        "        max_val = minmax_dict[key][1]\n",
        "      if scaler == \"normalize\":\n",
        "        processed_features[key] = linear_scale(value, min_val, max_val)\n",
        "      else:\n",
        "        processed_features[key] = linear_scale_htang(value, min_val, max_val)\n",
        "  return processed_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9ZtIDmGmT0i"
      },
      "source": [
        "#### Boxplot das variáveis de entrada linearmente normalizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0ngWUomUWk"
      },
      "source": [
        "#\n",
        "# code below just to show\n",
        "#\n",
        "\n",
        "# normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "# # normalized_all_examples_htang = normalize_linear_scale(all_examples, scaler=\"htang\")\n",
        "\n",
        "# print(\"All normalized examples summary:\")\n",
        "# display.display(normalized_all_examples.describe())\n",
        "\n",
        "# # Boxplot of variables through levels\n",
        "# for key in normalized_all_examples.columns.tolist():\n",
        "#     if key == \"k\": continue\n",
        "#     # plt.subplot(6, 2, idx_plot)\n",
        "#     normalized_all_examples.boxplot(column=key, by=\"k\", figsize=(15, 6))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcu2ymg3TOHX"
      },
      "source": [
        "#### Boxplot das variáveis de saída linearmente normalizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxXYXbe4TOHY"
      },
      "source": [
        "#\n",
        "# code below just to show\n",
        "#\n",
        "\n",
        "# normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "# print(\"All normalized targets summary:\")\n",
        "# display.display(normalized_all_targets.describe())\n",
        "\n",
        "# # Boxplot of variables through levels\n",
        "# for key in normalized_all_targets.columns.tolist():\n",
        "#     if key == \"k\": continue\n",
        "#     # plt.subplot(6, 2, idx_plot)\n",
        "#     normalized_all_targets.boxplot(column=key, by=\"k\", figsize=(15, 6))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ2z_NMRgk-D"
      },
      "source": [
        "### Normalização da base usando diferentes escalas para cada variável\n",
        "\n",
        "- Diferentes normalizações por variável, como linear, logarítimica e z-score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAVywqZwgk-J"
      },
      "source": [
        "def normalize_examples(examples_dataframe):\n",
        "  # \"k\", \"si\", \"sl\", \"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"tke\", \"kzh\", \"omega\"\n",
        "  processed_features = pd.DataFrame()\n",
        "\n",
        "  if len(examples_dataframe[\"k\"]) > 0:\n",
        "    processed_features[\"k\"] = examples_dataframe[\"k\"]\n",
        "\n",
        "  for m_var in [ \"si\", \"Tc\", \"qv\"]:\n",
        "    processed_features[m_var] = linear_scale(examples_dataframe[m_var], examples_dataframe[m_var].min(), examples_dataframe[m_var].max())\n",
        "\n",
        "  for m_var in [ \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"tke\", \"kzh\"]:\n",
        "    processed_features[m_var] = log_normalize(examples_dataframe[m_var])\n",
        "\n",
        "  processed_features[\"omega\"] = z_score_normalize(examples_dataframe[\"omega\"])\n",
        "  # processed_features[\"population\"] = linear_scale(clip(examples_dataframe[\"population\"], 0, 5000))\n",
        "    \n",
        "  return processed_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZKIz6DEmsmh"
      },
      "source": [
        "#### Boxplot das variáveis de entrada com diferentes normalizações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wdEt2mvms51"
      },
      "source": [
        "# normalized_all_examples = normalize_examples(all_examples)\n",
        "# normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "# print(\"All normalized examples summary:\")\n",
        "# display.display(normalized_all_examples.describe())\n",
        "\n",
        "# print(\"All normalized targets summary:\")\n",
        "# display.display(normalized_all_targets.describe())\n",
        "\n",
        "\n",
        "# # Boxplot of variables through levels\n",
        "# for key in normalized_all_examples.columns.tolist():\n",
        "#     if key == \"k\": continue\n",
        "#     # plt.subplot(6, 2, idx_plot)\n",
        "#     normalized_all_examples.boxplot(column=key, by=\"k\", figsize=(15, 6))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5MUmBleDwR"
      },
      "source": [
        "## Amostras dos Conjuntos de treinamento e validação\n",
        "\n",
        "Exibe  o resumo de entrada e saída de conjuntos de treinamento e validação, para verificar a homogeneidade desses conjuntos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUEuyouJ7pC0"
      },
      "source": [
        "def get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, train_percent_size):\n",
        "  # Choose the first x percent for training.\n",
        "  all_size = len(normalized_all_examples)\n",
        "  train_size = int(all_size * train_percent_size / k_max)*k_max\n",
        "  print(\"train size = {}\".format(train_size))\n",
        "\n",
        "  normalized_training_examples = normalized_all_examples.head(train_size)\n",
        "  normalized_training_targets = normalized_all_targets.head(train_size)\n",
        "\n",
        "  # Choose the last (10%) (out of 265468) for validation.\n",
        "  normalized_validation_examples = normalized_all_examples.iloc[train_size:]\n",
        "  normalized_validation_targets = normalized_all_targets.iloc[train_size:]\n",
        "  return normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets\n",
        "\n",
        "\n",
        "# versão atual que grava primeiro por variavel depois por nivel, e remove niveis de LSRAIN e LSSNOW\n",
        "def get_df_col_k(df_orig, k_inicial, k_final, dic_var_levs_exclude=None):\n",
        "  \n",
        "  print(df_orig.columns)\n",
        "  dic_var_levs_exclude_ok = { \\\n",
        "  'LSRAIN':range(2,29),\n",
        "  'LSSNOW':range(2,29)\n",
        "  }  \n",
        "  if dic_var_levs_exclude is not None:\n",
        "    dic_var_levs_exclude_ok.update(dic_var_levs_exclude)\n",
        "    \n",
        "  df_col_k = pd.DataFrame()\n",
        "  for col in df_orig.columns.tolist():\n",
        "    if col == 'k':\n",
        "        continue\n",
        "    for k in range(k_inicial, k_final + 1):\n",
        "      df = df_orig.copy()\n",
        "      df_k = df[df['k'] == k]\n",
        "      if col in dic_var_levs_exclude_ok.keys() and k in dic_var_levs_exclude_ok[col]:\n",
        "          continue\n",
        "      df_col_k['{}_k{}'.format(col, k)] = df_k[col].to_numpy()\n",
        "  return df_col_k\n",
        "\n",
        "\n",
        "# versão anterior que grava primeiro por nivel depois por variavel\n",
        "# def get_df_col_k(df_orig, k_inicial, k_final, dic_var_levs_exclude=None):\n",
        "#   df_col_k = pd.DataFrame()\n",
        "#   for k in range(k_inicial, k_final + 1):\n",
        "#     df = df_orig.copy()\n",
        "#     df_k = df[df['k'] == k]\n",
        "#     for col in df.columns.tolist():\n",
        "#       if col == 'k':\n",
        "#         continue\n",
        "#       if dic_var_levs_exclude is not None and col in dic_var_levs_exclude.keys():\n",
        "#         if k in dic_var_levs_exclude[col]:\n",
        "#           continue\n",
        "#       df_col_k['{}_k{}'.format(col, k)] = df_k[col].to_numpy()\n",
        "#   return df_col_k\n",
        "\n",
        "\n",
        "def get_arr_2D_col_k(df_orig, colname, k_inicial, k_final):\n",
        "  levels = k_final - k_inicial + 1\n",
        "  rows_one_level = int(len(df_orig.index)/levels)\n",
        "  arr_col_2d = np.zeros((rows_one_level, k_max))\n",
        "\n",
        "  for k in range(k_inicial, k_final + 1):\n",
        "    df = df_orig.copy()\n",
        "    df_k = df[df['k'] == k]\n",
        "    arr_col_2d[:, k-1] = df_k[colname].to_numpy()\n",
        " \n",
        "  return arr_col_2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUbhH8KDtGLg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d44394d7-4d0d-4c12-e794-73e53d6cd2e6"
      },
      "source": [
        "# Choose variables to train and validate\n",
        "# Eliminate variables are not varying\n",
        "# Separate 70% for training and 30# for validation\n",
        "\n",
        "training_examples_tmp, training_targets_tmp, validation_examples_tmp, validation_targets_tmp = get_percent_normalized_trainining_and_validation(all_examples, all_targets, 0.7)\n",
        "\n",
        "print(\"Training examples summary:\")\n",
        "display.display(training_examples_tmp.describe())\n",
        "\n",
        "print(\"Validation examples summary:\")\n",
        "display.display(validation_examples_tmp.describe())\n",
        "\n",
        "print(\"Training targets summary:\")\n",
        "display.display(training_targets_tmp.describe())\n",
        "\n",
        "print(\"Validation targets summary:\")\n",
        "display.display(validation_targets_tmp.describe())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size = 3405696\n",
            "Training examples summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>si</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>tke</th>\n",
              "      <th>kzh</th>\n",
              "      <th>omega</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>0.468551</td>\n",
              "      <td>237.594765</td>\n",
              "      <td>0.005161</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2255.438053</td>\n",
              "      <td>101151.914857</td>\n",
              "      <td>4068.123044</td>\n",
              "      <td>0.056920</td>\n",
              "      <td>101417.234712</td>\n",
              "      <td>0.454196</td>\n",
              "      <td>45.851484</td>\n",
              "      <td>-0.012420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.077748</td>\n",
              "      <td>0.363561</td>\n",
              "      <td>61.436343</td>\n",
              "      <td>0.005985</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>33985.727158</td>\n",
              "      <td>385104.416531</td>\n",
              "      <td>47210.180080</td>\n",
              "      <td>7.694274</td>\n",
              "      <td>877361.886257</td>\n",
              "      <td>1.409050</td>\n",
              "      <td>94.082704</td>\n",
              "      <td>0.078518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>63.503048</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>-0.985995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.750000</td>\n",
              "      <td>0.103880</td>\n",
              "      <td>202.663399</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.024188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>0.432240</td>\n",
              "      <td>257.231255</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.250000</td>\n",
              "      <td>0.834470</td>\n",
              "      <td>288.878719</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.125936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.698106</td>\n",
              "      <td>0.014214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>308.583864</td>\n",
              "      <td>0.016233</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>1145094.435400</td>\n",
              "      <td>2942266.541640</td>\n",
              "      <td>1521556.569890</td>\n",
              "      <td>6038.505010</td>\n",
              "      <td>35494575.117600</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>0.541837</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   k             si  ...            kzh          omega\n",
              "count 3405696.000000 3405696.000000  ... 3405696.000000 3405696.000000\n",
              "mean       14.500000       0.468551  ...      45.851484      -0.012420\n",
              "std         8.077748       0.363561  ...      94.082704       0.078518\n",
              "min         1.000000       0.003669  ...       0.030000      -0.985995\n",
              "25%         7.750000       0.103880  ...       0.100000      -0.024188\n",
              "50%        14.500000       0.432240  ...       0.100000       0.000000\n",
              "75%        21.250000       0.834470  ...      31.698106       0.014214\n",
              "max        28.000000       1.000000  ...     300.000000       0.541837\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation examples summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>si</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>tke</th>\n",
              "      <th>kzh</th>\n",
              "      <th>omega</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>0.468551</td>\n",
              "      <td>238.307706</td>\n",
              "      <td>0.005162</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2599.749378</td>\n",
              "      <td>133130.641273</td>\n",
              "      <td>3565.269513</td>\n",
              "      <td>0.022085</td>\n",
              "      <td>88587.819629</td>\n",
              "      <td>0.506581</td>\n",
              "      <td>51.573874</td>\n",
              "      <td>-0.010004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.077750</td>\n",
              "      <td>0.363561</td>\n",
              "      <td>61.153765</td>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>36647.852023</td>\n",
              "      <td>443906.088633</td>\n",
              "      <td>43928.512795</td>\n",
              "      <td>1.188425</td>\n",
              "      <td>807956.300147</td>\n",
              "      <td>1.505469</td>\n",
              "      <td>100.951058</td>\n",
              "      <td>0.080191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>73.449916</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>-1.076957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.750000</td>\n",
              "      <td>0.103880</td>\n",
              "      <td>202.356155</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.022978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>0.432240</td>\n",
              "      <td>257.704029</td>\n",
              "      <td>0.001508</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.250000</td>\n",
              "      <td>0.834470</td>\n",
              "      <td>289.448154</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.888145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>34.307073</td>\n",
              "      <td>0.017509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>308.962369</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000747</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>1145094.416000</td>\n",
              "      <td>2938224.984260</td>\n",
              "      <td>1462846.465110</td>\n",
              "      <td>221.915976</td>\n",
              "      <td>18389107.862400</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>0.396736</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   k             si  ...            kzh          omega\n",
              "count 1459612.000000 1459612.000000  ... 1459612.000000 1459612.000000\n",
              "mean       14.500000       0.468551  ...      51.573874      -0.010004\n",
              "std         8.077750       0.363561  ...     100.951058       0.080191\n",
              "min         1.000000       0.003669  ...       0.030000      -1.076957\n",
              "25%         7.750000       0.103880  ...       0.100000      -0.022978\n",
              "50%        14.500000       0.432240  ...       0.100000       0.000000\n",
              "75%        21.250000       0.834470  ...      34.307073       0.017509\n",
              "max        28.000000       1.000000  ...     300.000000       0.396736\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training targets summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>EFFCS</th>\n",
              "      <th>EFFIS</th>\n",
              "      <th>LSRAIN</th>\n",
              "      <th>LSSNOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "      <td>3405696.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>237.595058</td>\n",
              "      <td>0.005161</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2255.438053</td>\n",
              "      <td>101151.940145</td>\n",
              "      <td>4068.401709</td>\n",
              "      <td>0.056920</td>\n",
              "      <td>101422.048628</td>\n",
              "      <td>24.683494</td>\n",
              "      <td>27.500308</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.077748</td>\n",
              "      <td>61.435035</td>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>33985.727158</td>\n",
              "      <td>385104.411211</td>\n",
              "      <td>47211.505371</td>\n",
              "      <td>7.694274</td>\n",
              "      <td>877381.472657</td>\n",
              "      <td>2.304218</td>\n",
              "      <td>14.627737</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.503048</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.750000</td>\n",
              "      <td>202.664223</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>257.319978</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.250000</td>\n",
              "      <td>288.877419</td>\n",
              "      <td>0.009955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.128040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>308.583864</td>\n",
              "      <td>0.016276</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>1145094.435400</td>\n",
              "      <td>2942266.541640</td>\n",
              "      <td>1521556.569890</td>\n",
              "      <td>6038.505010</td>\n",
              "      <td>35494575.117600</td>\n",
              "      <td>27.581993</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   k             Tc  ...         LSRAIN         LSSNOW\n",
              "count 3405696.000000 3405696.000000  ... 3405696.000000 3405696.000000\n",
              "mean       14.500000     237.595058  ...       0.000001       0.000000\n",
              "std         8.077748      61.435035  ...       0.000004       0.000000\n",
              "min         1.000000      63.503048  ...      -0.000000      -0.000000\n",
              "25%         7.750000     202.664223  ...       0.000000       0.000000\n",
              "50%        14.500000     257.319978  ...       0.000000       0.000000\n",
              "75%        21.250000     288.877419  ...       0.000000       0.000000\n",
              "max        28.000000     308.583864  ...       0.000133       0.000000\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation targets summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Tc</th>\n",
              "      <th>qv</th>\n",
              "      <th>qc</th>\n",
              "      <th>qr</th>\n",
              "      <th>qi</th>\n",
              "      <th>qs</th>\n",
              "      <th>qg</th>\n",
              "      <th>ni</th>\n",
              "      <th>ns</th>\n",
              "      <th>nr</th>\n",
              "      <th>NG</th>\n",
              "      <th>NC</th>\n",
              "      <th>EFFCS</th>\n",
              "      <th>EFFIS</th>\n",
              "      <th>LSRAIN</th>\n",
              "      <th>LSSNOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "      <td>1459612.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>238.308012</td>\n",
              "      <td>0.005162</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2600.129198</td>\n",
              "      <td>133134.070509</td>\n",
              "      <td>3564.635900</td>\n",
              "      <td>0.022084</td>\n",
              "      <td>88576.587373</td>\n",
              "      <td>24.743197</td>\n",
              "      <td>27.248625</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.077750</td>\n",
              "      <td>61.152400</td>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>36650.691428</td>\n",
              "      <td>443911.023180</td>\n",
              "      <td>43925.182218</td>\n",
              "      <td>1.188425</td>\n",
              "      <td>807906.492990</td>\n",
              "      <td>2.046452</td>\n",
              "      <td>13.477777</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>73.449916</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.750000</td>\n",
              "      <td>202.356310</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.500000</td>\n",
              "      <td>257.744712</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.250000</td>\n",
              "      <td>289.447436</td>\n",
              "      <td>0.009953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.909875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>308.962369</td>\n",
              "      <td>0.016269</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000747</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>1145094.416000</td>\n",
              "      <td>2938224.984260</td>\n",
              "      <td>1462846.465110</td>\n",
              "      <td>221.915976</td>\n",
              "      <td>18389107.862400</td>\n",
              "      <td>26.671755</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   k             Tc  ...         LSRAIN         LSSNOW\n",
              "count 1459612.000000 1459612.000000  ... 1459612.000000 1459612.000000\n",
              "mean       14.500000     238.308012  ...       0.000001       0.000000\n",
              "std         8.077750      61.152400  ...       0.000003       0.000000\n",
              "min         1.000000      73.449916  ...      -0.000000      -0.000000\n",
              "25%         7.750000     202.356310  ...       0.000000       0.000000\n",
              "50%        14.500000     257.744712  ...       0.000000       0.000000\n",
              "75%        21.250000     289.447436  ...       0.000000       0.000000\n",
              "max        28.000000     308.962369  ...       0.000163       0.000000\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3CwLfstS24B"
      },
      "source": [
        "# Set using Variables per K\n",
        "# training_examples_tmp, training_targets_tmp, validation_examples_tmp, validation_targets_tmp = get_percent_normalized_trainining_and_validation(all_examples, all_targets, 0.7)\n",
        "\n",
        "# k_inicial = 1\n",
        "# k_final = k_max\n",
        "\n",
        "# training_examples_tmp = get_df_col_k(training_examples_tmp, k_inicial, k_final)\n",
        "# training_targets_tmp = get_df_col_k(training_targets_tmp, k_inicial, k_final)\n",
        "# validation_examples_tmp = get_df_col_k(validation_examples_tmp, k_inicial, k_final)\n",
        "# validation_targets_tmp = get_df_col_k(validation_targets_tmp, k_inicial, k_final)\n",
        "\n",
        "\n",
        "\n",
        "# display.display(training_examples_tmp.describe())\n",
        "# display.display(validation_examples_tmp.describe())\n",
        "# display.display(training_targets_tmp.describe())\n",
        "# display.display(validation_targets_tmp.describe())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqIbXxx222ea"
      },
      "source": [
        "# Treinamento da rede neural: Rotinas utilizadas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k3xYlSg27VB"
      },
      "source": [
        "def construct_feature_columns(input_features):\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Args:\n",
        "    input_features: The names of the numerical input features to use.\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  return set([tf.feature_column.numeric_column(my_feature)\n",
        "              for my_feature in input_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De9jwyy4wTUT"
      },
      "source": [
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Trains a neural network model.\n",
        "  \n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    print(\"Features:\", features)\n",
        "    print(\"Labels:\", labels)\n",
        "    return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHEjsYhZppdi"
      },
      "source": [
        "def use_tpu():\n",
        "\n",
        "# https://www.tensorflow.org/guide/tpu\n",
        "\n",
        "# eager é bom pra avaliar os objetos tensor sem precisar de graphs.\n",
        "# Nesse caso desativo pra não dar o erro Tensor.graph is meaningless when eager execution is enabled. \n",
        "\n",
        "  # tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "  try:\n",
        "\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "    # para TF2\n",
        "    # resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])  # https://www.tensorflow.org/guide/tpu\n",
        "    print('Running on TPU ', resolver.cluster_spec().as_dict()['worker'])\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  \n",
        "  # requer eager\n",
        "  # tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "  # This is the TPU initialization code that has to be at the beginning.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(resolver)\n",
        "  return tpu_strategy\n",
        "\n",
        "# tpu_strategy = use_tpu()\n",
        "# with tpu_strategy.scope():\n",
        "#  code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l73K0vebp2g2"
      },
      "source": [
        "def use_gpu():\n",
        "# Using GPU ...  mais lento que TPU - mesmo temo que CPU ou pouco mais rápido:\n",
        "# ex: \n",
        "#   INFO:tensorflow:global_step/sec: 299.778\n",
        "#   INFO:tensorflow:loss = 3.5140946, step = 200 (0.332 sec)\n",
        "  \n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# use_gpu()\n",
        "# with tf.device('/device:GPU:0') as gpu:\n",
        "#     print(\"Gpu name={}\".format(gpu))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12d1cDyWnSu6"
      },
      "source": [
        "def get_no_spinup_examples_and_targets(levels, delta_t, spin_hours, pre_process=False):\n",
        "\n",
        "  # TODO - original_*_df foi limpo pra limpar memória, após a sua última utilização em \n",
        "  # all_examples, all_targets. Tentar usar all_examples, all_targets\n",
        "\n",
        "  spinup_size = int(levels * spin_hours * 3600/delta_t) \n",
        "  total_size = len(original_input_df)\n",
        "  no_spinup_size = total_size-spinup_size\n",
        "  all_examples_tmp = original_input_df.tail(no_spinup_size)\n",
        "  all_targets_tmp = original_output_df.tail(no_spinup_size)\n",
        "  print(\"First 3 examples and targets for checking spinup start\")\n",
        "  display.display(all_examples_tmp.head(3))\n",
        "  display.display(all_targets_tmp.head(3))\n",
        "\n",
        "  if pre_process:\n",
        "    # select initial filtered variables\n",
        "    all_examples_tmp = preprocess_features(all_examples_tmp)\n",
        "    all_targets_tmp = preprocess_targets(all_targets_tmp)\n",
        "\n",
        "  return all_examples_tmp, all_targets_tmp, no_spinup_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvTwkne3e5kW"
      },
      "source": [
        "## Construção das NN customizáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RjWyylYe5kW"
      },
      "source": [
        "# get sample dataset\n",
        "def get_dataset():\n",
        "    X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=3, random_state=2)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def get_custom_model(n_inputs, n_outputs, neurons_per_layer, optimizer='adam', loss='mse'):\n",
        "    # dropout = Fraction of the input units to drop\n",
        "    dropout = 0\n",
        "    inputs = Input(shape=n_inputs)\n",
        "    x = inputs\n",
        "    for neurons in neurons_per_layer:\n",
        "        x = Dense(neurons, activation='relu')(x)\n",
        "    # TODO level_all = Dense(self.train_y.shape[1])(x)\n",
        "    level_all = Dense(n_outputs)(x)\n",
        "    model = Model(inputs=inputs, outputs=level_all)\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "  \n",
        "    return model\n",
        "\n",
        "\n",
        "def get_cnn_model(n_inputs, n_outputs, arr_convs_filters_kernels, neurons_per_layer, batch_norm=None, dropout=None, max_pooling=None, optimizer='adam', loss='mse', timesteps_window=None ):\n",
        "    # dropout = Fraction of the input units to drop\n",
        "  \n",
        "    # kernel = 3\n",
        "    # 448 - (kernel-1)\n",
        "    # 446 \n",
        "    model = Sequential()\n",
        "    for conv in arr_convs_filters_kernels:\n",
        "      if timesteps_window is None:\n",
        "        model.add(Conv1D(filters=conv[0], kernel_size=conv[1], activation='relu', input_shape=(n_inputs,1)))\n",
        "      else:\n",
        "        model.add(Conv1D(filters=conv[0], kernel_size=conv[1], activation='relu', input_shape=(timesteps_window, n_inputs)))\n",
        "      if batch_norm is not None:\n",
        "        # https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "    \n",
        "    if max_pooling is not None:\n",
        "      model.add(MaxPooling1D(pool_size=max_pooling))\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    for neurons in neurons_per_layer:\n",
        "        if neurons is not None:\n",
        "          model.add(Dense(neurons, activation='relu'))\n",
        "          if dropout is not None:\n",
        "            # https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html\n",
        "            model.add(Dropout(dropout))\n",
        "    \n",
        "    # precisa fazer o reshape quando for usar (caso com timesteps_window)\n",
        "    model.add(Dense(n_outputs))\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    return model\n",
        "\n",
        "\n",
        "# idem mas com Sequential - erro ?\n",
        "# def get_custom_model(n_inputs, n_outputs, neurons_per_layer, optimizer='adam', loss='mse'):\n",
        "\n",
        "#     model = keras.Sequential()\n",
        "#     first = True\n",
        "#     for neurons in neurons_per_layer:\n",
        "#       if first:\n",
        "#         model.add(Dense(neurons, input_dim=n_inputs, activation=\"relu\"))\n",
        "#         first = False\n",
        "#       else:\n",
        "#         model.add(Dense(neurons, activation=\"relu\"))\n",
        "\n",
        "#     model.add(Dense(n_outputs))\n",
        "#     model.compile(loss=loss, optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "#     return model\n",
        "    \n",
        "\n",
        "# get the model\n",
        "def get_sample_model(n_inputs, n_outputs, loss='mae'):\n",
        "    # losses = mean_squared_error, mae ... https://keras.io/api/losses/\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "    model.add(Dense(n_outputs))\n",
        "    model.compile(loss=loss, optimizer='adam')\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_curr_time():\n",
        "    return datetime.now().strftime(\"%m-%d.%H.%M\")\n",
        "\n",
        "\n",
        "def get_tb_logdir_var(test_name, layers, epochs, target_var=\"ALL_VARS\"):\n",
        "  log_dir_test = get_tb_logdir_test(test_name)\n",
        "  return \"{}/{}__{}-layers__{}-epochs___{}\".format(log_dir_test, target_var, layers, epochs, get_curr_time())\n",
        "\n",
        "\n",
        "def create_tb_logdir_test(test_name):\n",
        "  tb_logdir_test = get_tb_logdir_test(test_name)\n",
        "  rmtree(tb_logdir_test, ignore_errors=True)\n",
        "  os.makedirs(tb_logdir_test)\n",
        "  return tb_logdir_test\n",
        "\n",
        "\n",
        "def get_tb_logdir_test(test_name):\n",
        "  return \"{}/{}\".format(tb_logdir_base, test_name)\n",
        "\n",
        "\n",
        "def get_test_dir_name(test_name, n_inputs, hidden_layers, n_outputs, epochs, var_name):\n",
        "  test_dir_name = '{}_{}-inputs_{}-layers_{}-outputs_{}-epochs__{}'.format(test_name, n_inputs, hidden_layers, n_outputs, epochs, var_name)\n",
        "  return test_dir_name\n",
        "\n",
        "\n",
        "def get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, var_name):\n",
        "  save_dir='{}/saved_models'.format(colab_in_drive_root_dir)\n",
        "  model_var_name_dir = '{}/{}'.format(save_dir, get_test_dir_name(test_name, n_inputs, hidden_layers, n_outputs, epochs, var_name))\n",
        "  return model_var_name_dir\n",
        "\n",
        "\n",
        "def get_drive_test_log_dir(test_name):\n",
        "  drive_logs_test_dir = '{}/logs/{}'.format(colab_in_drive_root_dir, test_name)\n",
        "  return drive_logs_test_dir\n",
        "\n",
        "\n",
        "def create_drive_test_log_dir(test_name):\n",
        "  drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "  rmtree(drive_logs_test_dir, ignore_errors=True)\n",
        "  os.makedirs(drive_logs_test_dir)\n",
        "  return drive_logs_test_dir\n",
        "\n",
        "\n",
        "def save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir, var_name='ALL_VARS'):\n",
        "  model_var_name_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, var_name)\n",
        "  rmtree(model_var_name_dir, ignore_errors=True)\n",
        "  os.makedirs(model_var_name_dir)\n",
        "  log_msg('Saving model in {} ...'.format(model_var_name_dir))\n",
        "  \n",
        "  model.save(model_var_name_dir)\n",
        "  model.save(model_var_name_dir + '/tmp_model.h5')\n",
        "\n",
        "  plot_model(model, show_shapes=True, to_file='{}/model.png'.format(model_var_name_dir))\n",
        "\n",
        "  test_dir_name = get_test_dir_name(test_name, n_inputs, hidden_layers, n_outputs, epochs, var_name)\n",
        "  drive_logs_test_var_dir = '{}/{}'.format(drive_logs_test_dir, test_dir_name)\n",
        "  copytree(log_dir_var, drive_logs_test_var_dir)\n",
        "  log_msg('Saving logs in {} ...'.format(drive_logs_test_var_dir))\n",
        "\n",
        "\n",
        "# train and evaluate a model using repeated k-fold cross-validation\n",
        "def train_model(model, X, y, tb_log_dir, epochs=200, mode='split', n_splits=10, batch_size=64):\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir) \n",
        "    log_msg(\"Start training\")\n",
        "    start = time.time()\n",
        "    results = list()\n",
        "    # if len(X.shape) == 1:\n",
        "    #     n_inputs = 1\n",
        "    # else:\n",
        "    #     n_inputs = X.shape[1]\n",
        "    # if len(y.shape) == 1:\n",
        "    #     n_outputs = 1\n",
        "    # else:\n",
        "    #     n_outputs = y.shape[1]\n",
        "\n",
        "    if mode == 'k-folds':\n",
        "      # Using k-folds ---- bad- must use many splits (number of itens trained) - to slow\n",
        "      #\n",
        "      n_repeats = 2\n",
        "      epochs_k = int(epochs / n_splits)\n",
        "      cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
        "      first_time=True\n",
        "      # enumerate folds\n",
        "      for train_ix, test_ix in cv.split(X):\n",
        "          # prepare data\n",
        "          X_train, X_test = X[train_ix], X[test_ix]\n",
        "          y_train, y_test = y[train_ix], y[test_ix]\n",
        "          if first_time:\n",
        "            model.fit(X_train, y_train, verbose=1, epochs=epochs_k, callbacks=[tensorboard_callback], batch_size=batch_size, use_multiprocessing=True)\n",
        "            first_time = False\n",
        "          else:\n",
        "            model.fit(X_train, y_train, verbose=1, epochs=epochs_k, batch_size=batch_size, use_multiprocessing=True)\n",
        "          # evaluate model on test set\n",
        "          loss_metric = model.evaluate(X_test, y_test, verbose=1, batch_size=batch_size, use_multiprocessing=True)\n",
        "          log_msg( 'Test on TRAINING set. split {} of {} : [{} loss, metric(rmse)]={}'.format(spl, n_splits, model.loss, loss_metric))\n",
        "          # store result\n",
        "          results.append(loss_metric)\n",
        "\n",
        "    elif mode == 'split':\n",
        "      early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
        "\n",
        "      for spl in range(n_splits):\n",
        "        # Train / test split division (20% of total = 0.2/0.9 = 0.22222)\n",
        "        #\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "          X, y, test_size=(0.2/0.9), random_state=None)\n",
        "        display.display(X_train.shape)\n",
        "        display.display(X_test.shape)\n",
        "        display.display(y_train.shape)\n",
        "        display.display(y_test.shape)\n",
        "        \n",
        "        # model.fit(X_train, y_train, verbose=1, epochs=epochs, callbacks=[early_stop_callback, tensorboard_callback], batch_size=batch_size, use_multiprocessing=True)\n",
        "        model.fit(X_train, y_train, verbose=1, epochs=epochs, callbacks=[tensorboard_callback, early_stop_callback], batch_size=batch_size, use_multiprocessing=True)\n",
        "        \n",
        "        # evaluate model on test set\n",
        "        loss_metric = model.evaluate(X_test, y_test, verbose=1, batch_size=batch_size, use_multiprocessing=True)\n",
        "        log_msg( 'Test on TRAINING set. split {} of {} : [{} loss, metric(rmse)]={}'.format(spl+1, n_splits, model.loss, loss_metric))\n",
        "        results.append(loss_metric)\n",
        "\n",
        "    end = time.time()\n",
        "    log_msg(\"Training finished in {} seconds\".format(end-start))\n",
        "    log_msg( 'All training mse losses/rmse metric all spits: \\n {}'.format(results))\n",
        "    log_msg( '======> Test on TRAINING set: [{} loss, metric(rmse)]={}'.format(model.loss, loss_metric))\n",
        "    return results\n",
        "\n",
        "\n",
        "def validate_model(model, normalized_validation_examples, normalized_validation_targets, batch_size=64, is_CNN=False):\n",
        "  log_msg(\"Validating model in test set ...\")\n",
        "\n",
        "  arr_input_val = normalized_validation_examples.to_numpy()\n",
        "  if len(arr_input_val.shape) > 1:\n",
        "    num_vars = arr_input_val.shape[1]\n",
        "  else:\n",
        "    num_vars = 1\n",
        "  log_msg(\"Input Validation variables/size = {}/{}\".format(num_vars, arr_input_val.shape[0])) \n",
        "\n",
        "  if is_CNN:\n",
        "    arr_input_val = np.expand_dims(arr_input_val, 2)\n",
        "    \n",
        "  arr_output_val = normalized_validation_targets.to_numpy()\n",
        "  if len(arr_output_val.shape) > 1:\n",
        "    num_vars = arr_output_val.shape[1]\n",
        "  else:\n",
        "    num_vars = 1\n",
        "  log_msg(\"Output Validation variables/size = {}/{}\".format(num_vars, arr_output_val.shape[0])) \n",
        "\n",
        "  loss_metric_eval = model.evaluate(arr_input_val, arr_output_val, verbose=1, batch_size=batch_size, use_multiprocessing=True) \n",
        "  log_msg('=====> Final Test on VALIDATION set: [loss/rmse] = {}'.format(loss_metric_eval))\n",
        "\n",
        "\n",
        "def validate_model_cnn_timesteps(model, arr_input_val, arr_output_val, batch_size=64):\n",
        "  log_msg(\"Validating model in test set ...\")\n",
        "  loss_metric_eval = model.evaluate(arr_input_val, arr_output_val, verbose=1, batch_size=batch_size, use_multiprocessing=True) \n",
        "  log_msg('=====> Final Test on VALIDATION set: [loss/rmse] = {}'.format(loss_metric_eval))\n",
        "\n",
        "\n",
        "def log_msg(message):\n",
        "  log_file = '{}/log_messages.txt'.format(get_drive_test_log_dir(test_name))\n",
        "  f = open(log_file, 'a')\n",
        "  f.write('\\n' + message)\n",
        "  print(message)\n",
        " \n",
        "\n",
        "# sample execution\n",
        "# #\n",
        "# # load dataset\n",
        "# X, y = get_dataset()\n",
        "# n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "# print(n_inputs, n_outputs)\n",
        "# n_outputs\n",
        "# model = get_custom_model(n_inputs, n_outputs, [n_inputs, n_outputs]) #, 'mean_squared_error'\n",
        "# # model = get_sample_model(n_inputs, n_outputs, 'loss_metric')\n",
        "# test_name = 'kerastest'\n",
        "# drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "\n",
        "# # model.build()\n",
        "# model.summary(print_fn=log_msg)\n",
        "# plot_model(model, to_file='./model.png')\n",
        "# train_model(model, X, y, drive_logs_test_dir, epochs=100, n_splits=3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wGXnuQwr0S6"
      },
      "source": [
        "# Testes e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh36Nj15-2KO"
      },
      "source": [
        "## Rotinas para cálculo de métricas sobre os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M8RZ6Lr-nO6"
      },
      "source": [
        "#\n",
        "# rotinas para cálculo de métricas sobre os resultados\n",
        "#\n",
        "\n",
        "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Simple error \"\"\"\n",
        "    return actual - predicted\n",
        "\n",
        "\n",
        "def bias(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" BIAS \"\"\"\n",
        "    return np.mean(_error(actual, predicted))\n",
        "\n",
        "\n",
        "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Mean Squared Error \"\"\"\n",
        "    return np.mean(np.square(_error(actual, predicted)))\n",
        "\n",
        "\n",
        "def rmse(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Root Mean Squared Error \"\"\"\n",
        "    return np.sqrt(mse(actual, predicted))\n",
        "\n",
        "\n",
        "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Mean Absolute Error \"\"\"\n",
        "    return np.mean(np.abs(_error(actual, predicted)))\n",
        "\n",
        "\n",
        "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\"\n",
        "    Percentage error\n",
        "    Note: result is NOT multiplied by 100\n",
        "    \"\"\"\n",
        "    return _error(actual, predicted) / (actual + EPSILON)\n",
        "\n",
        "    \n",
        "def mpe(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Mean Percentage Error \"\"\"\n",
        "    return np.mean(_percentage_error(actual, predicted))\n",
        "\n",
        "\n",
        "def mape(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\"\n",
        "    Mean Absolute Percentage Error\n",
        "    Properties:\n",
        "        + Easy to interpret\n",
        "        + Scale independent\n",
        "        - Biased, not symmetric\n",
        "        - Undefined when actual[t] == 0\n",
        "    Note: result is NOT multiplied by 100\n",
        "    \"\"\"\n",
        "    return np.mean(np.abs(_percentage_error(actual, predicted)))\n",
        "\n",
        "\n",
        "def rmspe(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\"\n",
        "    Root Mean Squared Percentage Error\n",
        "    Note: result is NOT multiplied by 100\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean(np.square(_percentage_error(actual, predicted))))\n",
        "\n",
        "\n",
        "def write_header_B(normalized_all_targets, writer):\n",
        "  df_tmp = normalized_all_targets.copy()\n",
        "  if 'k' in df_tmp: \n",
        "    df_tmp.drop(columns=['k'], inplace=True)\n",
        "  writer.writerow(df_tmp.columns)\n",
        "  df_tmp = None\n",
        "\n",
        "\n",
        "def get_rmse_csv_writer():\n",
        "  return get_csv_writer('log_rmse_tmp.csv')\n",
        "\n",
        "\n",
        "def get_csv_writer(fname):\n",
        "  log_file_name = '{}/{}'.format(get_drive_test_log_dir(test_name), fname)\n",
        "  file = open(log_file_name, 'a+')\n",
        "  writer = csv.writer(file)\n",
        "  return writer, file\n",
        "\n",
        "\n",
        "def get_mean_csv_writer():\n",
        "  log_file_name = '{}/log_mean_tmp.csv'.format(get_drive_test_log_dir(test_name))\n",
        "  file = open(log_file_name, 'a+')\n",
        "  writer = csv.writer(file)\n",
        "  return writer, file\n",
        "\n",
        "\n",
        "def calc_rmse_B(normalized_validation_examples, normalized_validation_targets, writer):\n",
        "  # validation on every line of output file\n",
        "\n",
        "  arr_input_val = normalized_validation_examples.to_numpy()\n",
        "  arr_output_val = normalized_validation_targets.to_numpy()\n",
        "\n",
        "  log_msg(\"predicting {} registers \".format(arr_input_val.shape[0]))\n",
        "  start = time.time()\n",
        "  y = model.predict(arr_input_val, use_multiprocessing=True)\n",
        "  end = time.time()\n",
        "  log_msg(\"Prediction time MULTIPROCESSING in {} seconds\".format(end-start))\n",
        "\n",
        "  line_rmse = []\n",
        "  for col_idx in range(arr_output_val.shape[1]):\n",
        "    line_rmse.append(rmse(y[:,col_idx], arr_output_val[:,col_idx]))\n",
        "  writer.writerow(line_rmse)\n",
        "\n",
        "  # just to calculate prediction of 1 pass into routine ...\n",
        "\n",
        "  arr_input_val = normalized_validation_examples.tail(k_max).to_numpy()\n",
        "  log_msg(\"predicting {} registers \".format(arr_input_val.shape[0]))\n",
        "  start = time.time()\n",
        "  y = model.predict(arr_input_val, use_multiprocessing=True)\n",
        "  end = time.time()\n",
        "  log_msg(\"Prediction time of 1 pass into routine MULTIPROCESSING in {} seconds\".format(end-start))\n",
        "\n",
        "\n",
        "def calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values, dic_var_default_values=None, is_CNN=False):\n",
        "    \n",
        "  writer_rmse, frmse = get_rmse_csv_writer()\n",
        "  fbias_name = 'log_bias_tmp.csv'\n",
        "  writer_bias, fbias = get_csv_writer(fbias_name)\n",
        "  fmse_name = 'log_mse_tmp.csv'\n",
        "  writer_mse, fmse = get_csv_writer(fmse_name)\n",
        "  fmae_name = 'log_mae_tmp.csv'\n",
        "  writer_mae, fmae = get_csv_writer(fmae_name)\n",
        "  fmean_pred_name = 'log_mean_pred_tmp.csv'\n",
        "  writer_mean_pred, fmean_pred = get_csv_writer(fmean_pred_name)\n",
        "  fmean_obs_name = 'log_mean_obs_tmp.csv'\n",
        "  writer_mean_obs, fmean_obs = get_csv_writer(fmean_obs_name)\n",
        "\n",
        "  write_header_B(normalized_validation_targets, writer_rmse)\n",
        "  write_header_B(normalized_validation_targets, writer_mse)\n",
        "  write_header_B(normalized_validation_targets, writer_bias)\n",
        "  write_header_B(normalized_validation_targets, writer_mae)\n",
        "  write_header_B(normalized_validation_targets, writer_mean_pred)\n",
        "  write_header_B(normalized_validation_targets, writer_mean_obs)\n",
        "\n",
        "  arr_input_val = normalized_validation_examples.to_numpy()\n",
        "  if is_CNN:\n",
        "    arr_input_val = np.expand_dims(arr_input_val, 2)\n",
        "  arr_output_val = normalized_validation_targets.to_numpy()\n",
        "\n",
        "  log_msg(\"predicting {} registers \".format(arr_input_val.shape[0]))\n",
        "  start = time.time()\n",
        "  y = model.predict(arr_input_val, use_multiprocessing=True)\n",
        "  end = time.time()\n",
        "  log_msg(\"Prediction time MULTIPROCESSING in {} seconds\".format(end-start))\n",
        "\n",
        "  line_bias = []\n",
        "  line_mae = []\n",
        "  line_rmse = []\n",
        "  line_mse = []\n",
        "  line_mean_pred = []\n",
        "  line_mean_obs = []\n",
        "  for col_idx in range(arr_output_val.shape[1]):\n",
        "    line_mse.append(mse(arr_output_val[:,col_idx], y[:,col_idx]))\n",
        "    line_rmse.append(rmse(arr_output_val[:,col_idx], y[:,col_idx]))\n",
        "    line_bias.append(bias(arr_output_val[:,col_idx], y[:,col_idx]))\n",
        "    line_mae.append(mae(arr_output_val[:,col_idx], y[:,col_idx]))\n",
        "    line_mean_pred.append(np.average(y[:,col_idx]))\n",
        "    line_mean_obs.append(np.average(arr_output_val[:,col_idx]))\n",
        "\n",
        "  \n",
        "  writer_rmse.writerow(line_rmse)\n",
        "  writer_mse.writerow(line_mse)\n",
        "  writer_bias.writerow(line_bias)\n",
        "  writer_mae.writerow(line_mae)\n",
        "  writer_mean_pred.writerow(line_mean_pred)\n",
        "  writer_mean_obs.writerow(line_mean_obs)\n",
        "\n",
        "  frmse.close()\n",
        "  fmse.close()\n",
        "  fbias.close()\n",
        "  fmae.close()\n",
        "  fmean_pred.close()\n",
        "  fmean_obs.close()\n",
        "\n",
        "  # convert cols to rows, putting zeros in rows without values\n",
        "  #\n",
        "  create_files_csv(all_targets_min_values, all_targets_max_values, 'log_rmse_tmp.csv', 'log_rmse.csv', 'log_rmse_denorm.csv', dic_var_default_values=dic_var_default_values)\n",
        "  create_files_csv(all_targets_min_values, all_targets_max_values, fbias_name, 'log_bias.csv', 'log_bias_denorm.csv', dic_var_default_values=dic_var_default_values)\n",
        "  create_files_csv(all_targets_min_values, all_targets_max_values, fmae_name, 'log_mae.csv', 'log_mae_denorm.csv', dic_var_default_values=dic_var_default_values)\n",
        "  create_files_csv(all_targets_min_values, all_targets_max_values, fmse_name, 'log_mse.csv', 'log_mse_denorm.csv', dic_var_default_values=dic_var_default_values)  \n",
        "  create_files_csv(all_targets_min_values, all_targets_max_values, fmean_pred_name, 'log_mean_pred.csv', 'log_mean_pred_denorm.csv', scale_or_delinear='delinear', dic_var_default_values=dic_var_default_values)\n",
        "  create_files_csv(all_targets_min_values, all_targets_max_values, fmean_obs_name, 'log_mean_obs.csv', 'log_mean_obs_denorm.csv', scale_or_delinear='delinear', dic_var_default_values=dic_var_default_values)\n",
        "\n",
        "  # just to calculate prediction of 1 pass into routine ...\n",
        "  arr_input_val = normalized_validation_examples.tail(k_max).to_numpy()\n",
        "  if is_CNN:\n",
        "    arr_input_val = np.expand_dims(arr_input_val, 2)\n",
        "  log_msg(\"predicting {} registers \".format(arr_input_val.shape[0]))\n",
        "  start = time.time()\n",
        "  y = model.predict(arr_input_val, use_multiprocessing=True)\n",
        "  end = time.time()\n",
        "  log_msg(\"Prediction time of 1 pass into routine MULTIPROCESSING in {} seconds\".format(end-start))\n",
        "\n",
        "\n",
        "def create_files_csv(min_values, max_values, log_tmp_file, new_log_filename, new_log_denorm_filename, scale_or_delinear='scale', dic_var_default_values=None):\n",
        "  log_file_name = '{}/{}'.format(get_drive_test_log_dir(test_name), log_tmp_file)\n",
        "  df = pd.read_csv(log_file_name)\n",
        "  all_out_columns = [\"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"EFFCS\", \"EFFIS\", \"LSRAIN\", \"LSSNOW\"]\n",
        "  df_t = pd.DataFrame(columns=all_out_columns)\n",
        "  for col_name in all_out_columns:\n",
        "      if scale_or_delinear == 'delinear' and dic_var_default_values is not None and col_name in dic_var_default_values.keys():\n",
        "        df_t[col_name] = [dic_var_default_values[col_name]] * k_max\n",
        "      else:\n",
        "        df_t[col_name] = [0.0] * k_max\n",
        "  df_t_denorm = df_t.copy()\n",
        "  for col in df.columns:\n",
        "      col_name_k = col.split(\"_k\")\n",
        "      col_name = col_name_k[0]\n",
        "      k = int(col_name_k[1])\n",
        "      df_t[col_name].iloc[k-1] = df[col].values[0]\n",
        "      if scale_or_delinear == 'scale':\n",
        "        df_t_denorm[col_name].iloc[k-1] = df[col].values[0] * (max_values[col_name] - min_values[col_name])\n",
        "      else:  # delinear scale\n",
        "        df_t_denorm[col_name].iloc[k-1] = delinear_scale(df[col].values[0], min_values[col_name], max_values[col_name])\n",
        "  \n",
        "  df_t.to_csv('{}/{}'.format(get_drive_test_log_dir(test_name), new_log_filename))\n",
        "  df_t_denorm.to_csv('{}/{}'.format(get_drive_test_log_dir(test_name), new_log_denorm_filename))\n",
        "\n",
        "\n",
        "def calc_rmse_A(normalized_validation_examples, normalized_validation_targets, writer, df_rmse, target_key):\n",
        "  # validation on every line of output file\n",
        "\n",
        "  line_rmse = []\n",
        "  for k in range(1,29):\n",
        "    df_examples = normalized_validation_examples.copy()\n",
        "    df_examples_k = df_examples[df_examples['k'] == k]\n",
        "    df_targets = normalized_validation_targets.copy()\n",
        "    df_targets_k = df_targets[df_targets['k'] == k]\n",
        "    df_targets_k.drop(columns=['k'], inplace=True)\n",
        "\n",
        "    arr_input_val = df_examples_k.to_numpy()\n",
        "    arr_output_val = df_targets_k.to_numpy()\n",
        "\n",
        "    log_msg(\"predicting {} registers \".format(arr_input_val.shape[0]))\n",
        "    start = time.time()\n",
        "    y = model.predict(arr_input_val, use_multiprocessing=True)\n",
        "    end = time.time()\n",
        "    log_msg(\"Prediction time MULTIPROCESSING in {} seconds\".format(end-start))\n",
        "    print('rmse = ', rmse(y, arr_output_val))\n",
        "    line_rmse.append(rmse(y, arr_output_val))\n",
        "    \n",
        "  df_rmse[target_key] = line_rmse\n",
        "\n",
        "  return df_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHMvpuO7AR-K"
      },
      "source": [
        "# all_out_columns = [\"Tc\", \"qv\", \"qc\", \"qr\", \"qi\", \"qs\", \"qg\", \"ni\", \"ns\", \"nr\", \"NG\", \"NC\", \"EFFCS\", \"EFFIS\", \"LSRAIN\", \"LSSNOW\"]\n",
        "# df_t = pd.DataFrame(columns=all_out_columns)\n",
        "\n",
        "# ???????\n",
        "# df_t[\"Tc\"]=[1,2]\n",
        "# df_t[\"Tc\"]=[1,2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKaboaogAR-K"
      },
      "source": [
        "## Preparação para os testes Arquitetura C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhqZuskYAR-L"
      },
      "source": [
        "def get_data_for_test(all_examples, all_targets, train_percent_size=0.9):\n",
        "\n",
        "    all_minmax_values = {}\n",
        "    for key in all_examples.columns:\n",
        "      all_minmax_values[key] = [all_examples[key].min(), all_examples[key].max()]\n",
        "\n",
        "    for key in all_targets.columns:\n",
        "      if key not in all_minmax_values.keys():\n",
        "        all_minmax_values[key] = [all_targets[key].min(), all_targets[key].max()]\n",
        "      else:\n",
        "        if all_targets[key].min() < all_minmax_values[key][0]:\n",
        "          all_minmax_values[key][0] = all_targets[key].min()\n",
        "        if all_targets[key].max() > all_minmax_values[key][1]:\n",
        "          all_minmax_values[key][1] = all_targets[key].max()\n",
        "        \n",
        "    all_min_values = {k: all_minmax_values[k][0] for k in all_minmax_values.keys()}\n",
        "    all_max_values = {k: all_minmax_values[k][1] for k in all_minmax_values.keys()}\n",
        "\n",
        "    print('\\n  all minmax values: ')\n",
        "    print(all_minmax_values)\n",
        "    \n",
        "    normalized_all_examples = normalize_linear_scale(all_examples, all_minmax_values)\n",
        "    normalized_all_targets = normalize_linear_scale(all_targets, all_minmax_values)\n",
        "\n",
        "    normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "    get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, train_percent_size)\n",
        "    \n",
        "    return normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ6PNUSNFmc5"
      },
      "source": [
        "Após a execução dos testes, os logs ficam armazenados no google drive, para que não se percam na sessão do colab. Outro notebook foi criado para visualizar os resultados no Tensorboard.  \n",
        "=>  https://colab.research.google.com/drive/1RiIXCWdcpGTdSLimPI3mJDWl7nj5fm1j?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPbzJz08c5yk"
      },
      "source": [
        "## C) Testes com (k_max níveis x num_var) inputs e (k_max níveis x num_var outputs) (Uma predição por passagem na rotina)\n",
        "\n",
        "- Utiliza 70% da base para treinamento, 20% para validação interna e 10% para validação independente\n",
        "- Normalização linear em todas as var de entrada ou outras normalizações\n",
        "- Faz a regressão para todas as variáveis de saída, para cada nível "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmjtPoyZ_7vn"
      },
      "source": [
        "##Testes IOP 2014 (D) ##\n",
        "\n",
        "- Idem C mas com IOP 2014\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- camada de saída com 16 variáveis x k_max níveis -27 níveis LSRAIN LSSONW =  394 outnputs\n",
        "- Executar a rede neural com normalização linear\n",
        "- Utilizando esquema de 10 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKwR-pZRd5On"
      },
      "source": [
        "levels = k_max\n",
        "delta_t = 180\n",
        "spin_hours = 24\n",
        "k_inicial = 1\n",
        "k_final = k_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNtVCIQQTsRY"
      },
      "source": [
        "### Testes de números de neurônios\n",
        "- Utilizando 10% ou 90% da base\n",
        "- 500 passos\n",
        "- Variando Neurônios, camadas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA0vZjDHTsR6",
        "outputId": "2d5ff121-2550-4fc4-f624-e992bb7d99cc"
      },
      "source": [
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "get_data_for_test(all_examples, all_targets, 0.9)\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "del normalized_training_examples, normalized_training_targets\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "\n",
        "\n",
        "# for hidden_layers in [[18], [36], [54], [72], [90], [108], [216], [432], \n",
        "#                       [18, 18], [36, 36], [54, 54], [72, 72], [90, 90], [108, 108], [216, 216], [432, 432], [448, 394], \n",
        "#                       [18, 18, 18], [36, 36, 36], [54, 54, 54], [72, 72, 72], [90, 90, 90], [108, 108, 108],[216, 216, 216], [432, 432, 432] ]:\n",
        "for hidden_layers in [ [216], [432], [108, 108], [216, 216], [432, 432], [108, 108, 108],[216, 216, 216], [432, 432, 432]]:\n",
        "  test_name = \"Evaluate_D_90_{}\".format(hidden_layers)\n",
        "  drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "  # drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "  log_msg(\" =========== Executando teste {} ==================\".format(test_name))\n",
        "\n",
        "\n",
        "  model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "  model.summary(print_fn=log_msg)\n",
        "\n",
        "  log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "  log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "  model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "  # Train model\n",
        "  log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "  train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=1)\n",
        "  save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "  # use trained model\n",
        "  # model = load_model((model_dir + '/tmp_model.h5'))\n",
        "\n",
        "  validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "  dic_var_default_values = { \\\n",
        "    'qc': 0.0,\n",
        "    'qr': 0.0,\n",
        "    'qi': 0.0,\n",
        "    'qs': 0.0,\n",
        "    'ni': 0.0,\n",
        "    'ns': 0.0,\n",
        "    'nr': 0.0,\n",
        "    'NG': 0.0,\n",
        "    'NC': 0.0,\n",
        "    'EFFCS': 0.891634,\n",
        "    'EFFIS': 0.123289\t\n",
        "  }\n",
        "  calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values, dic_var_default_values)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " =========== Executando teste Evaluate_D_90_[216] ==================\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 448)]             0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 216)               96984     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 394)               85498     \n",
            "=================================================================\n",
            "Total params: 182,482\n",
            "Trainable params: 182,482\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Input  train variables/size = 448/156384\n",
            "Output train variables/size = 394/156384 \n",
            "Start training\n",
            "Epoch 1/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 0.0054 - root_mean_squared_error: 0.0640\n",
            "Epoch 2/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 8.7957e-04 - root_mean_squared_error: 0.0297\n",
            "Epoch 3/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 7.9740e-04 - root_mean_squared_error: 0.0282\n",
            "Epoch 4/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 7.4661e-04 - root_mean_squared_error: 0.0273\n",
            "Epoch 5/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 7.3248e-04 - root_mean_squared_error: 0.0271\n",
            "Epoch 6/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.9256e-04 - root_mean_squared_error: 0.0263\n",
            "Epoch 7/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.7647e-04 - root_mean_squared_error: 0.0260\n",
            "Epoch 8/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.7644e-04 - root_mean_squared_error: 0.0260\n",
            "Epoch 9/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.7680e-04 - root_mean_squared_error: 0.0260\n",
            "Epoch 10/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.7281e-04 - root_mean_squared_error: 0.0259\n",
            "Epoch 11/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.5838e-04 - root_mean_squared_error: 0.0257\n",
            "Epoch 12/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.5935e-04 - root_mean_squared_error: 0.0257\n",
            "Epoch 13/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.5517e-04 - root_mean_squared_error: 0.0256\n",
            "Epoch 14/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.4172e-04 - root_mean_squared_error: 0.0253\n",
            "Epoch 15/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.3871e-04 - root_mean_squared_error: 0.0253\n",
            "Epoch 16/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.3598e-04 - root_mean_squared_error: 0.0252\n",
            "Epoch 17/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.2098e-04 - root_mean_squared_error: 0.0249\n",
            "Epoch 18/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.2443e-04 - root_mean_squared_error: 0.0250\n",
            "Epoch 19/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.2211e-04 - root_mean_squared_error: 0.0249\n",
            "Epoch 20/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.2005e-04 - root_mean_squared_error: 0.0249\n",
            "Epoch 21/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.1671e-04 - root_mean_squared_error: 0.0248\n",
            "Epoch 22/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.1748e-04 - root_mean_squared_error: 0.0248\n",
            "Epoch 23/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.9932e-04 - root_mean_squared_error: 0.0245\n",
            "Epoch 24/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 6.0707e-04 - root_mean_squared_error: 0.0246\n",
            "Epoch 25/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.9805e-04 - root_mean_squared_error: 0.0245\n",
            "Epoch 26/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.9403e-04 - root_mean_squared_error: 0.0244\n",
            "Epoch 27/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.9288e-04 - root_mean_squared_error: 0.0243\n",
            "Epoch 28/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.8690e-04 - root_mean_squared_error: 0.0242\n",
            "Epoch 29/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.8843e-04 - root_mean_squared_error: 0.0243\n",
            "Epoch 30/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.8491e-04 - root_mean_squared_error: 0.0242\n",
            "Epoch 31/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.7798e-04 - root_mean_squared_error: 0.0240\n",
            "Epoch 32/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.7755e-04 - root_mean_squared_error: 0.0240\n",
            "Epoch 33/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.8354e-04 - root_mean_squared_error: 0.0242\n",
            "Epoch 34/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.8062e-04 - root_mean_squared_error: 0.0241\n",
            "Epoch 35/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.7477e-04 - root_mean_squared_error: 0.0240\n",
            "Epoch 36/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.6150e-04 - root_mean_squared_error: 0.0237\n",
            "Epoch 37/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.6878e-04 - root_mean_squared_error: 0.0238\n",
            "Epoch 38/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.6411e-04 - root_mean_squared_error: 0.0238\n",
            "Epoch 39/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.6805e-04 - root_mean_squared_error: 0.0238\n",
            "Epoch 40/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.6532e-04 - root_mean_squared_error: 0.0238\n",
            "Epoch 41/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.5757e-04 - root_mean_squared_error: 0.0236\n",
            "Epoch 42/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.6337e-04 - root_mean_squared_error: 0.0237\n",
            "Epoch 43/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.6082e-04 - root_mean_squared_error: 0.0237\n",
            "Epoch 44/500\n",
            "1901/1901 [==============================] - 6s 3ms/step - loss: 5.5120e-04 - root_mean_squared_error: 0.0235\n",
            "Epoch 45/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.4961e-04 - root_mean_squared_error: 0.0234\n",
            "Epoch 46/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.5839e-04 - root_mean_squared_error: 0.0236\n",
            "Epoch 47/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.5854e-04 - root_mean_squared_error: 0.0236\n",
            "Epoch 48/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.4996e-04 - root_mean_squared_error: 0.0235\n",
            "Epoch 49/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.5064e-04 - root_mean_squared_error: 0.0235\n",
            "Epoch 50/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.4157e-04 - root_mean_squared_error: 0.0233\n",
            "Epoch 51/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.4325e-04 - root_mean_squared_error: 0.0233\n",
            "Epoch 52/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.4556e-04 - root_mean_squared_error: 0.0234\n",
            "Epoch 53/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.4029e-04 - root_mean_squared_error: 0.0232\n",
            "Epoch 54/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.4970e-04 - root_mean_squared_error: 0.0234\n",
            "Epoch 55/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.4541e-04 - root_mean_squared_error: 0.0234\n",
            "Epoch 56/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.4155e-04 - root_mean_squared_error: 0.0233\n",
            "Epoch 57/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.3127e-04 - root_mean_squared_error: 0.0230\n",
            "Epoch 58/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.3458e-04 - root_mean_squared_error: 0.0231\n",
            "Epoch 59/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.2953e-04 - root_mean_squared_error: 0.0230\n",
            "Epoch 60/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.4056e-04 - root_mean_squared_error: 0.0232\n",
            "Epoch 61/500\n",
            "1901/1901 [==============================] - 7s 3ms/step - loss: 5.3535e-04 - root_mean_squared_error: 0.0231\n",
            "Epoch 62/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.3428e-04 - root_mean_squared_error: 0.0231\n",
            "Epoch 63/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.3352e-04 - root_mean_squared_error: 0.0231\n",
            "Epoch 64/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.2245e-04 - root_mean_squared_error: 0.0229\n",
            "Epoch 65/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.1561e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 66/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.3256e-04 - root_mean_squared_error: 0.0231\n",
            "Epoch 67/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.3232e-04 - root_mean_squared_error: 0.0231\n",
            "Epoch 68/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.2395e-04 - root_mean_squared_error: 0.0229\n",
            "Epoch 69/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.1825e-04 - root_mean_squared_error: 0.0228\n",
            "Epoch 70/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.1957e-04 - root_mean_squared_error: 0.0228\n",
            "Epoch 71/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.1632e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 72/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.2499e-04 - root_mean_squared_error: 0.0229\n",
            "Epoch 73/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.2307e-04 - root_mean_squared_error: 0.0229\n",
            "Epoch 74/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.1610e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 75/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1046e-04 - root_mean_squared_error: 0.0226\n",
            "Epoch 76/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.2052e-04 - root_mean_squared_error: 0.0228\n",
            "Epoch 77/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.2388e-04 - root_mean_squared_error: 0.0229\n",
            "Epoch 78/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1569e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 79/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1181e-04 - root_mean_squared_error: 0.0226\n",
            "Epoch 80/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1255e-04 - root_mean_squared_error: 0.0226\n",
            "Epoch 81/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0401e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 82/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0618e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 83/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1354e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 84/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0411e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 85/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 5.0544e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 86/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0726e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 87/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0804e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 88/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0770e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 89/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 5.0782e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 90/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0305e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 91/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0589e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 92/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1429e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 93/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0830e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 94/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0841e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 95/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0797e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 96/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0448e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 97/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0013e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 98/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.1322e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 99/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0271e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 100/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0359e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 101/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9911e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 102/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 5.0771e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 103/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0363e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 104/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9923e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 105/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0389e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 106/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0189e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 107/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9940e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 108/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9913e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 109/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0005e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 110/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9182e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 111/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9614e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 112/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9724e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 113/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 5.0049e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 114/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 5.0248e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 115/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9658e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 116/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9311e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 117/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8572e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 118/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8788e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 119/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9760e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 120/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9384e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 121/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8883e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 122/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8682e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 123/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9184e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 124/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8530e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 125/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8363e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 126/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9149e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 127/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9402e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 128/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8860e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 129/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8618e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 130/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8503e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 131/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8657e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 132/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.9277e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 133/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8095e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 134/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8710e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 135/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8696e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 136/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8995e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 137/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8635e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 138/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8943e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 139/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8200e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 140/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.7841e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 141/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8122e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 142/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.8500e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 143/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.7576e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 144/500\n",
            "1901/1901 [==============================] - 9s 4ms/step - loss: 4.7673e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 145/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8540e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 146/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9220e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 147/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8945e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 148/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8598e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 149/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8670e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 150/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8548e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 151/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8320e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 152/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8589e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 153/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8415e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 154/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7845e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 155/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8379e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 156/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8118e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 157/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8174e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 158/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8127e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 159/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8046e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 160/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8637e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 161/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7727e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 162/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8803e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 163/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7928e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 164/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6786e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 165/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8003e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 166/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7572e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 167/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7793e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 168/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8035e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 169/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8093e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 170/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7562e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 171/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8187e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 172/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7808e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 173/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8097e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 174/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7599e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 175/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7853e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 176/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7476e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 177/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.9004e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 178/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8175e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 179/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7255e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 180/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7655e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 181/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.8021e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 182/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7425e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 183/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7415e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 184/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7329e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 185/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6678e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 186/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7213e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 187/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7037e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 188/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7505e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 189/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7351e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 190/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7385e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 191/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7197e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 192/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7774e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 193/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7376e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 194/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7311e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 195/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6636e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 196/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7828e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 197/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6926e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 198/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6378e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 199/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7254e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 200/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7164e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 201/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7420e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 202/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7009e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 203/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.9047e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 204/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6995e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 205/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6844e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 206/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7552e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 207/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7649e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 208/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6923e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 209/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7738e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 210/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6013e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 211/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7213e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 212/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6446e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 213/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6228e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 214/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6455e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 215/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6402e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 216/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5899e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 217/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6579e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 218/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6639e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 219/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7409e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 220/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6450e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 221/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5768e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 222/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6859e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 223/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6144e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 224/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6354e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 225/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7193e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 226/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6925e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 227/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6661e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 228/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6203e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 229/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6180e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 230/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6335e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 231/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7116e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 232/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5413e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 233/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6645e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 234/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6432e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 235/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5981e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 236/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6842e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 237/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6709e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 238/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6464e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 239/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6610e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 240/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6312e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 241/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6835e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 242/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7330e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 243/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6526e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 244/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7841e-04 - root_mean_squared_error: 0.0219\n",
            "Epoch 245/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6554e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 246/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6425e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 247/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6523e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 248/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6347e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 249/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6114e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 250/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6786e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 251/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7380e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 252/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7111e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 253/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6068e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 254/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6359e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 255/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7107e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 256/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6452e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 257/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6901e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 258/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6316e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 259/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7619e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 260/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6868e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 261/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6356e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 262/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7338e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 263/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6775e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 264/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7399e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 265/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6368e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 266/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6424e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 267/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6873e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 268/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6963e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 269/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5909e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 270/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6959e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 271/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6704e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 272/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6921e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 273/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6390e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 274/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6345e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 275/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7040e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 276/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6599e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 277/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6067e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 278/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6308e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 279/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6848e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 280/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5895e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 281/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6782e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 282/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6988e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 283/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7066e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 284/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6958e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 285/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6986e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 286/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7048e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 287/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7170e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 288/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.7090e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 289/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5408e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 290/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6412e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 291/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6911e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 292/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6513e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 293/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7159e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 294/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6759e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 295/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6039e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 296/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7102e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 297/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7478e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 298/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6521e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 299/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6550e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 300/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6199e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 301/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6326e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 302/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5660e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 303/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6501e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 304/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6169e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 305/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.7416e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 306/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6337e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 307/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5979e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 308/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6540e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 309/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6219e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 310/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6161e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 311/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6472e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 312/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5724e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 313/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6010e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 314/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6649e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 315/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5368e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 316/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5439e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 317/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5601e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 318/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6313e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 319/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5726e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 320/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5609e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 321/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5641e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 322/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6300e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 323/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5772e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 324/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6062e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 325/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6559e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 326/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5868e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 327/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6501e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 328/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6635e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 329/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5338e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 330/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5800e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 331/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.6799e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 332/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6295e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 333/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5516e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 334/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6010e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 335/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4945e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 336/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5588e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 337/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5880e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 338/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6012e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 339/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5942e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 340/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5603e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 341/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5994e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 342/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5597e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 343/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6440e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 344/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5075e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 345/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5191e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 346/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5097e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 347/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4587e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 348/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5769e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 349/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5892e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 350/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5868e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 351/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5514e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 352/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6189e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 353/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6520e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 354/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5410e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 355/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6497e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 356/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6018e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 357/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5541e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 358/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5551e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 359/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6081e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 360/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4939e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 361/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5483e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 362/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5031e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 363/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5680e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 364/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.6136e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 365/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4795e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 366/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4873e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 367/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5034e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 368/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5625e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 369/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4968e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 370/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5741e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 371/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5693e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 372/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5805e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 373/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5036e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 374/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4569e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 375/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4924e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 376/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3927e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 377/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5053e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 378/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4152e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 379/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4491e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 380/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4796e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 381/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4339e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 382/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5208e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 383/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5151e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 384/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5259e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 385/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4246e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 386/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4856e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 387/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.4423e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 388/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.5335e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 389/500\n",
            "1901/1901 [==============================] - 7s 4ms/step - loss: 4.4854e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 390/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4266e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 391/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4635e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 392/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4001e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 393/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5326e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 394/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4812e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 395/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5640e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 396/500\n",
            "1901/1901 [==============================] - 9s 4ms/step - loss: 4.4737e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 397/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.5065e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 398/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.4426e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 399/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.4783e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 400/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.4304e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 401/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4660e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 402/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4506e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 403/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5173e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 404/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3919e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 405/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5001e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 406/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3538e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 407/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4380e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 408/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4349e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 409/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3833e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 410/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4147e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 411/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3996e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 412/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4729e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 413/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4185e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 414/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4659e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 415/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4828e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 416/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4147e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 417/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4214e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 418/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4321e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 419/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3772e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 420/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4550e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 421/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4718e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 422/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3990e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 423/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4756e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 424/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4313e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 425/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3782e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 426/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4035e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 427/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4180e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 428/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4068e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 429/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4734e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 430/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4435e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 431/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4147e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 432/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3958e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 433/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4648e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 434/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3707e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 435/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4119e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 436/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4403e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 437/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4531e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 438/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3830e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 439/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4870e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 440/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3354e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 441/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4809e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 442/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3736e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 443/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.5211e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 444/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4206e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 445/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4106e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 446/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3220e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 447/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4544e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 448/500\n",
            "1901/1901 [==============================] - 9s 4ms/step - loss: 4.4498e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 449/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4506e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 450/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4492e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 451/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3426e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 452/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3732e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 453/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4410e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 454/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3631e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 455/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3634e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 456/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4345e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 457/500\n",
            "1901/1901 [==============================] - 9s 5ms/step - loss: 4.4270e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 458/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4027e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 459/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4996e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 460/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4750e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 461/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4016e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 462/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3932e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 463/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3876e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 464/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4168e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 465/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4040e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 466/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3731e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 467/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3758e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 468/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3479e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 469/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4000e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 470/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.2961e-04 - root_mean_squared_error: 0.0207\n",
            "Epoch 471/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3655e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 472/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4355e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 473/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3916e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 474/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3670e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 475/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4387e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 476/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3602e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 477/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4329e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 478/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3899e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 479/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3043e-04 - root_mean_squared_error: 0.0207\n",
            "Epoch 480/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4107e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 481/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3667e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 482/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4407e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 483/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3008e-04 - root_mean_squared_error: 0.0207\n",
            "Epoch 484/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4436e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 485/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3516e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 486/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3503e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 487/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4078e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 488/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3771e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 489/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4112e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 490/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3537e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 491/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3331e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 492/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3163e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 493/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4437e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 494/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3435e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 495/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3494e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 496/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3044e-04 - root_mean_squared_error: 0.0207\n",
            "Epoch 497/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4232e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 498/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.4046e-04 - root_mean_squared_error: 0.0210\n",
            "Epoch 499/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3326e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 500/500\n",
            "1901/1901 [==============================] - 8s 4ms/step - loss: 4.3391e-04 - root_mean_squared_error: 0.0208\n",
            "543/543 [==============================] - 1s 2ms/step - loss: 4.8662e-04 - root_mean_squared_error: 0.0221\n",
            "Test on TRAINING set. split 1 of 1 : [mse loss, metric(rmse)]=[0.00048661758773960173, 0.02205941081047058]\n",
            "Training finished in 3886.0854482650757 seconds\n",
            "All training mse losses/rmse metric all spits: \n",
            " [[0.00048661758773960173, 0.02205941081047058]]\n",
            "======> Test on TRAINING set: [mse loss, metric(rmse)]=[0.00048661758773960173, 0.02205941081047058]\n",
            "Saving model in /content/drive/My Drive/Colab Notebooks//saved_models/Evaluate_D_90_[216]_448-inputs_[216]-layers_394-outputs_500-epochs__ALL_VARS ...\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks//saved_models/Evaluate_D_90_[216]_448-inputs_[216]-layers_394-outputs_500-epochs__ALL_VARS/assets\n",
            "Saving logs in /content/drive/My Drive/Colab Notebooks//logs/Evaluate_D_90_[216]/Evaluate_D_90_[216]_448-inputs_[216]-layers_394-outputs_500-epochs__ALL_VARS ...\n",
            "Validating model in test set ...\n",
            "Input Validation variables/size = 448/17377\n",
            "Output Validation variables/size = 394/17377\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.0013 - root_mean_squared_error: 0.0356\n",
            "=====> Final Test on VALIDATION set: [loss/rmse] = [0.0012703357497230172, 0.03564176335930824]\n",
            "predicting 17377 registers \n",
            "Prediction time MULTIPROCESSING in 0.801414966583252 seconds\n",
            "predicting 28 registers \n",
            "Prediction time of 1 pass into routine MULTIPROCESSING in 0.03957557678222656 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WOLQ2Fl_Pud"
      },
      "source": [
        "Testes utilizando CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Q62FrLnjNH"
      },
      "source": [
        "### Teste CNN.6\n",
        "- Idem C.6 mas com CNN\n",
        "- 1 feature com 448 timesteps ?\n",
        "- Conv1D sobre os níveis ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XTc9UQsYnjNq",
        "outputId": "4de0c683-6afb-4521-80a1-900c24294469"
      },
      "source": [
        "# TODO\n",
        "# all_spin_examples, all_spin_targets, no_spinup_size = get_no_spinup_examples_and_targets(levels, delta_t, spin_hours, pre_process=True)    \n",
        "# normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "# get_data_for_test(all_spin_examples, all_spin_targets)\n",
        "\n",
        "use_levs = 'NoLevs'\n",
        "\n",
        "# ********* agilizando ...\n",
        "\n",
        "# normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "# get_data_for_test(all_examples, all_targets, 0.9)\n",
        "\n",
        "# if use_levs == 'Levs':\n",
        "#   dic_var_levels_exclude = { \\\n",
        "#     # 'sl': range(1,29),                          \n",
        "#     'qv': range(20,29),\n",
        "#     'qc': list(range(1,10)) + list(range(14,29)),\n",
        "#     'qr': range(14,29),\n",
        "#     'qi': list(range(1,12)) + list(range(20,29)),\n",
        "#     'ni': list(range(1,11)) + list(range(20,29)),\n",
        "#     'ns': list(range(1,4)) + [22, 29],\n",
        "#     'nr': range(14,29),\n",
        "#     'NC': list(range(1,9)) + list(range(14,29)),\n",
        "#     'omega': range(23,29)\n",
        "#   }\n",
        "# else:\n",
        "#   dic_var_levels_exclude = None\n",
        "\n",
        "# normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "# normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "# normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "# normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "# arr_input_train = normalized_training_examples.to_numpy()\n",
        "# arr_output_train = normalized_training_targets.to_numpy()\n",
        "# # https://stackoverflow.com/questions/48140989/keras-lstm-input-dimension-setting\n",
        "# # o mínimo de dimensões para a Conv1D são 3, então:\n",
        "# #  n_inputs, 448 =>  n_inputs, 448 , 1\n",
        "# arr_input_train = np.expand_dims(arr_input_train, 2)\n",
        "# arr_output_train = np.expand_dims(arr_output_train, 2)\n",
        "# del normalized_training_examples, normalized_training_targets\n",
        "# n_inputs = arr_input_train.shape[1]\n",
        "# n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "\n",
        "epochs=100\n",
        "n_splits=5\n",
        "\n",
        "\n",
        "dropout = None\n",
        "batch_norm_arr = [None]\n",
        "n_filters = [8, 16, 32]\n",
        "n_kernels = [5]\n",
        "# hidden_layers_arr = [[108]]\n",
        "hidden_layers_arr = [[None]]\n",
        "max_pooling = None\n",
        "batch_norm = None\n",
        "\n",
        "for hidden_layers in hidden_layers_arr:\n",
        "# #   for nf in n_filters:\n",
        "# #     for nk in n_kernels:\n",
        "# #       # for arr_convs_filters_kernels in [ [[nf, nk]], [[nf, nk], [nf, nk]], [[nf, nk], [nf, nk], [nf, nk]] ]:\n",
        "# #       for arr_convs_filters_kernels in [ [[nf, nk]], [[nf, nk], [nf, nk]] ]:\n",
        "# #         for batch_norm in batch_norm_arr:        \n",
        "# #           for max_pooling in max_pooling_arr:\n",
        "#             # Testar mais de uma camada pra tentar melhorar, mas com menos filtros pra reduzir parametros\n",
        "\n",
        "#   for arr_convs_filters_kernels in [ [[8, 5]] ]:\n",
        "  # for nf in n_filters:\n",
        "  #   for nk in n_kernels:\n",
        "  for arr_convs_filters_kernels in [ [[8, 5], [16, 5]], [[8, 5], [32, 5]], [[16, 5], [8, 5]], [[16, 5], [32, 5]], [[32, 5], [8, 5]], [[32, 5], [16, 5]] ]:\n",
        "    test_name = \"CNN_{}_filtKern_{}__pool_{}__hidd_{}__batch_{}__drop_{}\".format(use_levs, arr_convs_filters_kernels, max_pooling, hidden_layers, batch_norm, dropout) \n",
        "    drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "    if os.path.exists(drive_logs_test_dir): continue\n",
        "\n",
        "    drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "    log_msg(\" =========== Executando teste {} ==================\".format(test_name))\n",
        "\n",
        "    model = get_cnn_model(n_inputs, n_outputs, arr_convs_filters_kernels, hidden_layers, batch_norm, dropout, max_pooling, loss=loss_type) \n",
        "    model.summary(print_fn=log_msg)\n",
        "\n",
        "    log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "    log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "    model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "    # Train model\n",
        "    log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "\n",
        "    # tpu_strategy = use_tpu()\n",
        "    # with tpu_strategy.scope():\n",
        "\n",
        "    # ********* agilizando ...\n",
        "    use_gpu()\n",
        "    with tf.device('/device:GPU:0') as gpu:\n",
        "      train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=n_splits)\n",
        "\n",
        "    save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "    # ********* agilizando ...\n",
        "    # use trained model\n",
        "    # model = load_model((model_dir + '/tmp_model.h5'))\n",
        "\n",
        "    validate_model(model, normalized_validation_examples, normalized_validation_targets, is_CNN=True)\n",
        "    dic_var_default_values = { \\\n",
        "      'qc': 0.0,\n",
        "      'qr': 0.0,\n",
        "      'qi': 0.0,\n",
        "      'qs': 0.0,\n",
        "      'ni': 0.0,\n",
        "      'ns': 0.0,\n",
        "      'nr': 0.0,\n",
        "      'NG': 0.0,\n",
        "      'NC': 0.0,\n",
        "      'EFFCS': 0.891634,\n",
        "      'EFFIS': 0.123289\t\n",
        "    }\n",
        "    calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values, dic_var_default_values, is_CNN=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " =========== Executando teste CNN_NoLevs_filtKern_[[8, 5], [16, 5]]__pool_None__hidd_[None]__batch_None__drop_None ==================\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_10 (Conv1D)           (None, 444, 8)            48        \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, 440, 16)           656       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 7040)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 394)               2774154   \n",
            "=================================================================\n",
            "Total params: 2,774,858\n",
            "Trainable params: 2,774,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Input  train variables/size = 448/156384\n",
            "Output train variables/size = 394/156384 \n",
            "Found GPU at: /device:GPU:0\n",
            "Start training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(121632, 448, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(34752, 448, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(121632, 394, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(34752, 394, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1901/1901 [==============================] - 14s 7ms/step - loss: 0.0036 - root_mean_squared_error: 0.0523\n",
            "Epoch 2/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 7.1961e-04 - root_mean_squared_error: 0.0268\n",
            "Epoch 3/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 6.6308e-04 - root_mean_squared_error: 0.0257\n",
            "Epoch 4/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 6.3937e-04 - root_mean_squared_error: 0.0253\n",
            "Epoch 5/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 5.9762e-04 - root_mean_squared_error: 0.0244\n",
            "Epoch 6/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 5.9060e-04 - root_mean_squared_error: 0.0243\n",
            "Epoch 7/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.7966e-04 - root_mean_squared_error: 0.0241\n",
            "Epoch 8/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.6908e-04 - root_mean_squared_error: 0.0239\n",
            "Epoch 9/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.5528e-04 - root_mean_squared_error: 0.0236\n",
            "Epoch 10/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.3738e-04 - root_mean_squared_error: 0.0232\n",
            "Epoch 11/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.3976e-04 - root_mean_squared_error: 0.0232\n",
            "Epoch 12/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.2464e-04 - root_mean_squared_error: 0.0229\n",
            "Epoch 13/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.1751e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 14/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.0939e-04 - root_mean_squared_error: 0.0226\n",
            "Epoch 15/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 5.0359e-04 - root_mean_squared_error: 0.0224\n",
            "Epoch 16/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 4.9933e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 17/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.9150e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 18/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.8823e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 19/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.7667e-04 - root_mean_squared_error: 0.0218\n",
            "Epoch 20/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.8826e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 21/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.7245e-04 - root_mean_squared_error: 0.0217\n",
            "Epoch 22/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.6256e-04 - root_mean_squared_error: 0.0215\n",
            "Epoch 23/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.6778e-04 - root_mean_squared_error: 0.0216\n",
            "Epoch 24/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.5881e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 25/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.5847e-04 - root_mean_squared_error: 0.0214\n",
            "Epoch 26/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.4643e-04 - root_mean_squared_error: 0.0211\n",
            "Epoch 27/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.5237e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 28/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.5284e-04 - root_mean_squared_error: 0.0213\n",
            "Epoch 29/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.4924e-04 - root_mean_squared_error: 0.0212\n",
            "Epoch 30/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3227e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3806e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 32/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3756e-04 - root_mean_squared_error: 0.0209\n",
            "Epoch 33/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3148e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3261e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3420e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.3204e-04 - root_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.2697e-04 - root_mean_squared_error: 0.0207\n",
            "Epoch 38/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.2267e-04 - root_mean_squared_error: 0.0206\n",
            "Epoch 39/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1916e-04 - root_mean_squared_error: 0.0205\n",
            "Epoch 40/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1287e-04 - root_mean_squared_error: 0.0203\n",
            "Epoch 41/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1115e-04 - root_mean_squared_error: 0.0203\n",
            "Epoch 42/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1372e-04 - root_mean_squared_error: 0.0203\n",
            "Epoch 43/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1160e-04 - root_mean_squared_error: 0.0203\n",
            "Epoch 44/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1038e-04 - root_mean_squared_error: 0.0203\n",
            "Epoch 45/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.1767e-04 - root_mean_squared_error: 0.0204\n",
            "Epoch 46/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.0244e-04 - root_mean_squared_error: 0.0201\n",
            "Epoch 47/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.0315e-04 - root_mean_squared_error: 0.0201\n",
            "Epoch 48/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.0165e-04 - root_mean_squared_error: 0.0200\n",
            "Epoch 49/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 4.0516e-04 - root_mean_squared_error: 0.0201\n",
            "Epoch 50/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.9868e-04 - root_mean_squared_error: 0.0200\n",
            "Epoch 51/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.9789e-04 - root_mean_squared_error: 0.0199\n",
            "Epoch 52/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.9232e-04 - root_mean_squared_error: 0.0198\n",
            "Epoch 53/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.8727e-04 - root_mean_squared_error: 0.0197\n",
            "Epoch 54/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.9166e-04 - root_mean_squared_error: 0.0198\n",
            "Epoch 55/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.9368e-04 - root_mean_squared_error: 0.0198\n",
            "Epoch 56/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.8116e-04 - root_mean_squared_error: 0.0195\n",
            "Epoch 57/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.8771e-04 - root_mean_squared_error: 0.0197\n",
            "Epoch 58/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.8966e-04 - root_mean_squared_error: 0.0197\n",
            "Epoch 59/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.8289e-04 - root_mean_squared_error: 0.0196\n",
            "Epoch 60/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.7886e-04 - root_mean_squared_error: 0.0195\n",
            "Epoch 61/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.7856e-04 - root_mean_squared_error: 0.0195\n",
            "Epoch 62/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.8476e-04 - root_mean_squared_error: 0.0196\n",
            "Epoch 63/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.7019e-04 - root_mean_squared_error: 0.0192\n",
            "Epoch 64/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6572e-04 - root_mean_squared_error: 0.0191\n",
            "Epoch 65/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.7494e-04 - root_mean_squared_error: 0.0194\n",
            "Epoch 66/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.7544e-04 - root_mean_squared_error: 0.0194\n",
            "Epoch 67/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6786e-04 - root_mean_squared_error: 0.0192\n",
            "Epoch 68/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.7013e-04 - root_mean_squared_error: 0.0192\n",
            "Epoch 69/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6367e-04 - root_mean_squared_error: 0.0191\n",
            "Epoch 70/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6352e-04 - root_mean_squared_error: 0.0191\n",
            "Epoch 71/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6593e-04 - root_mean_squared_error: 0.0191\n",
            "Epoch 72/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6218e-04 - root_mean_squared_error: 0.0190\n",
            "Epoch 73/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6721e-04 - root_mean_squared_error: 0.0192\n",
            "Epoch 74/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6129e-04 - root_mean_squared_error: 0.0190\n",
            "Epoch 75/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.5389e-04 - root_mean_squared_error: 0.0188\n",
            "Epoch 76/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6022e-04 - root_mean_squared_error: 0.0190\n",
            "Epoch 77/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5904e-04 - root_mean_squared_error: 0.0189\n",
            "Epoch 78/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6604e-04 - root_mean_squared_error: 0.0191\n",
            "Epoch 79/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.6088e-04 - root_mean_squared_error: 0.0190\n",
            "Epoch 80/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5289e-04 - root_mean_squared_error: 0.0188\n",
            "Epoch 81/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5471e-04 - root_mean_squared_error: 0.0188\n",
            "Epoch 82/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5425e-04 - root_mean_squared_error: 0.0188\n",
            "Epoch 83/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4848e-04 - root_mean_squared_error: 0.0187\n",
            "Epoch 84/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4472e-04 - root_mean_squared_error: 0.0186\n",
            "Epoch 85/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5035e-04 - root_mean_squared_error: 0.0187\n",
            "Epoch 86/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4519e-04 - root_mean_squared_error: 0.0186\n",
            "Epoch 87/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5151e-04 - root_mean_squared_error: 0.0187\n",
            "Epoch 88/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4700e-04 - root_mean_squared_error: 0.0186\n",
            "Epoch 89/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.3932e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 90/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4302e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 91/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4213e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 92/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3473e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 93/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4851e-04 - root_mean_squared_error: 0.0187\n",
            "Epoch 94/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4406e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 95/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.5148e-04 - root_mean_squared_error: 0.0187\n",
            "Epoch 96/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4734e-04 - root_mean_squared_error: 0.0186\n",
            "Epoch 97/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4371e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 98/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4696e-04 - root_mean_squared_error: 0.0186\n",
            "Epoch 99/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.4311e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 100/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4750e-04 - root_mean_squared_error: 0.0186\n",
            "543/543 [==============================] - 2s 3ms/step - loss: 3.4704e-04 - root_mean_squared_error: 0.0186\n",
            "Test on TRAINING set. split 1 of 5 : [mse loss, metric(rmse)]=[0.00034703928395174444, 0.018628988415002823]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(121632, 448, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(34752, 448, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(121632, 394, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(34752, 394, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.4461e-04 - root_mean_squared_error: 0.0186\n",
            "Epoch 2/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4361e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 3/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4283e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 4/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4196e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 5/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4118e-04 - root_mean_squared_error: 0.0185\n",
            "Epoch 6/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.4038e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 7/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3984e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 8/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3900e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 9/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3851e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 10/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3758e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 11/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3712e-04 - root_mean_squared_error: 0.0184\n",
            "Epoch 12/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3616e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 13/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3580e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 14/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3524e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 15/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.3441e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 16/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3399e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 17/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.3367e-04 - root_mean_squared_error: 0.0183\n",
            "Epoch 18/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.3299e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 19/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.3210e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 20/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3192e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 21/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.3121e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 22/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.3062e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 23/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.2978e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 24/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2962e-04 - root_mean_squared_error: 0.0182\n",
            "Epoch 25/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2903e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 26/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2879e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 27/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.2840e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 28/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2734e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 29/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2739e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 30/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2715e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 31/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.2590e-04 - root_mean_squared_error: 0.0181\n",
            "Epoch 32/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2530e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 33/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2536e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 34/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2484e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 35/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2420e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 36/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2401e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 37/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2345e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 38/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2318e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 39/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2260e-04 - root_mean_squared_error: 0.0180\n",
            "Epoch 40/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2200e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 41/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2190e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 42/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.2095e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 43/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.2034e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 44/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.2096e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 45/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.2032e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 46/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.1955e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 47/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.1860e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 48/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1887e-04 - root_mean_squared_error: 0.0179\n",
            "Epoch 49/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1839e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 50/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.1823e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 51/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1762e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 52/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1745e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 53/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1746e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 54/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1639e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 55/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1606e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 56/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1598e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 57/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1585e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 58/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1515e-04 - root_mean_squared_error: 0.0178\n",
            "Epoch 59/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1471e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 60/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1460e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 61/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1402e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 62/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1310e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 63/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1378e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 64/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1309e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 65/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1258e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 66/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1320e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 67/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1195e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 68/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1177e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 69/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1166e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 70/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1184e-04 - root_mean_squared_error: 0.0177\n",
            "Epoch 71/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1066e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 72/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1047e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 73/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1126e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 74/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.1019e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 75/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.0962e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 76/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.1059e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 77/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0930e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 78/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0953e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 79/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0852e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 80/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0845e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 81/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0873e-04 - root_mean_squared_error: 0.0176\n",
            "Epoch 82/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0786e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 83/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0797e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 84/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0716e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 85/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0723e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 86/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0721e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 87/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0676e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 88/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0710e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 89/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0607e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 90/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0619e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 91/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0508e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 92/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0560e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 93/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0544e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 94/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0522e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 95/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.0508e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 96/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0517e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 97/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0493e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 98/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0418e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 99/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0452e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 100/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.0350e-04 - root_mean_squared_error: 0.0174\n",
            "543/543 [==============================] - 1s 3ms/step - loss: 3.0065e-04 - root_mean_squared_error: 0.0173\n",
            "Test on TRAINING set. split 2 of 5 : [mse loss, metric(rmse)]=[0.00030065380269661546, 0.017339374870061874]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(121632, 448, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(34752, 448, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(121632, 394, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(34752, 394, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1901/1901 [==============================] - 13s 7ms/step - loss: 3.0523e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 2/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 3.0458e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 3/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0419e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 4/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0335e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 5/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0450e-04 - root_mean_squared_error: 0.0175\n",
            "Epoch 6/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0345e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 7/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0278e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 8/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0234e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 9/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0289e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 10/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0173e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 11/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0131e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 12/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0138e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 13/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0211e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 14/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0137e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 15/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0108e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 16/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0115e-04 - root_mean_squared_error: 0.0174\n",
            "Epoch 17/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0058e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 18/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0014e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 19/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9946e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 20/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 3.0015e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 21/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9961e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 22/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9919e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 23/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9898e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 24/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9874e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 25/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9794e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 26/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9824e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 27/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9859e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 28/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9792e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 29/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9743e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 30/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9795e-04 - root_mean_squared_error: 0.0173\n",
            "Epoch 31/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9706e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 32/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9737e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 33/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9677e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 34/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9697e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 35/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9671e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 36/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9723e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 37/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9647e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 38/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9637e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 39/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9552e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 40/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9607e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 41/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9602e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 42/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9593e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 43/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9534e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 44/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9514e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 45/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9504e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 46/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9418e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 47/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9438e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 48/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9405e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 49/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9401e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 50/100\n",
            "1901/1901 [==============================] - 12s 7ms/step - loss: 2.9405e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 51/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9420e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 52/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9380e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 53/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9417e-04 - root_mean_squared_error: 0.0172\n",
            "Epoch 54/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9288e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 55/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9337e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 56/100\n",
            "1901/1901 [==============================] - 12s 6ms/step - loss: 2.9292e-04 - root_mean_squared_error: 0.0171\n",
            "Epoch 57/100\n",
            "1822/1901 [===========================>..] - ETA: 0s - loss: 2.9306e-04 - root_mean_squared_error: 0.0171"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUsLpH6LiCOG"
      },
      "source": [
        "### Teste CNN com timesteps\n",
        "- 448 features de entrada\n",
        "- n timesteps onde ocorre a conv1D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRsQBkD9u2oN"
      },
      "source": [
        "# a = np.arange(30)\n",
        "# a1 = a.reshape(10,3)\n",
        "\n",
        "# new_samples=5\n",
        "# timesteps_window=2\n",
        "# a2=a1.reshape(new_samples, timesteps_window, a1.shape[1])\n",
        "# a3=a1.reshape(new_samples, timesteps_window * a1.shape[1])\n",
        "# display.display(a1)\n",
        "# display.display(a2)\n",
        "# display.display(a3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GAzEQvhiCOW",
        "outputId": "dbcbb351-eb5c-476d-8630-28cdd1a2dcc1"
      },
      "source": [
        "# TODO\n",
        "# all_spin_examples, all_spin_targets, no_spinup_size = get_no_spinup_examples_and_targets(levels, delta_t, spin_hours, pre_process=True)    \n",
        "# normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "# get_data_for_test(all_spin_examples, all_spin_targets)\n",
        "\n",
        "use_levs = 'NoLevs'\n",
        "\n",
        "# ********* agilizando ...\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "get_data_for_test(all_examples, all_targets, 0.9)\n",
        "if use_levs == 'Levs':\n",
        "  dic_var_levels_exclude = { \\\n",
        "    # 'sl': range(1,29),                          \n",
        "    'qv': range(20,29),\n",
        "    'qc': list(range(1,10)) + list(range(14,29)),\n",
        "    'qr': range(14,29),\n",
        "    'qi': list(range(1,12)) + list(range(20,29)),\n",
        "    'ni': list(range(1,11)) + list(range(20,29)),\n",
        "    'ns': list(range(1,4)) + [22, 29],\n",
        "    'nr': range(14,29),\n",
        "    'NC': list(range(1,9)) + list(range(14,29)),\n",
        "    'omega': range(23,29)\n",
        "  }\n",
        "else:\n",
        "  dic_var_levels_exclude = None\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "# ********* fim agilizando ...\n",
        "\n",
        "# del normalized_training_examples, normalized_training_targets\n",
        "\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "arr_input_val = normalized_validation_examples.to_numpy()\n",
        "arr_output_val = normalized_validation_targets.to_numpy()\n",
        "\n",
        "\n",
        "timesteps_window=2\n",
        "\n",
        "# TRAIN\n",
        "new_samples=int(arr_input_train.shape[0]/timesteps_window)\n",
        "arr_input_train = arr_input_train[0:new_samples*timesteps_window,:]\n",
        "arr_input_train = arr_input_train.reshape(new_samples, timesteps_window, arr_input_train.shape[1])\n",
        "arr_output_train = arr_output_train[0:new_samples*timesteps_window,:]\n",
        "arr_output_train = arr_output_train.reshape(new_samples, timesteps_window * arr_output_train.shape[1])\n",
        "\n",
        "# VALIDATION\n",
        "new_samples = int(arr_input_val.shape[0]/timesteps_window)\n",
        "arr_input_val = arr_input_val[0:new_samples*timesteps_window,:]\n",
        "arr_input_val = arr_input_val.reshape(new_samples, timesteps_window, arr_input_val.shape[1])\n",
        "arr_output_val = arr_output_val[0:new_samples*timesteps_window,:]\n",
        "arr_output_val = arr_output_val.reshape(new_samples, timesteps_window * arr_output_val.shape[1])\n",
        "\n",
        "n_inputs = arr_input_train.shape[2]\n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "\n",
        "epochs=100\n",
        "n_splits=5\n",
        "\n",
        "\n",
        "dropout = None\n",
        "# for hidden_layers in [ [216], [432], [216, 216], [432, 432] ]:  \n",
        "# hidden_layers_arr = [ [None], [216], [432], [216, 216], [432, 432] ]\n",
        "hidden_layers_arr = [ [None], [108] ]\n",
        "batch_norm_arr = [None]\n",
        "max_pooling_arr = [None, 2]\n",
        "n_filters = [32, 64]\n",
        "\n",
        "# melhor 5\n",
        "n_kernels = [2]\n",
        "batch_norm = None\n",
        "  # arr_convs_filters_kernels = [ [32, 3] ]\n",
        "\n",
        "for hidden_layers in hidden_layers_arr:\n",
        "  for nf in n_filters:\n",
        "    for nk in n_kernels:\n",
        "      for max_pooling in max_pooling_arr:\n",
        "        # for arr_convs_filters_kernels in [ [[nf, nk]], [[nf, nk], [nf, nk]], [[nf, nk], [nf, nk], [nf, nk]] ]:\n",
        "        for arr_convs_filters_kernels in [ [[nf, nk]] ]:\n",
        "#         for batch_norm in batch_norm_arr:        \n",
        "\n",
        "\n",
        "          test_name = \"CNN_TIMESTEPS__{}_filtKern_{}__pool_{}__hidd_{}__batch_{}__drop_{}__timesteps_{}\".format(use_levs, arr_convs_filters_kernels, max_pooling, hidden_layers, batch_norm, dropout, timesteps_window) \n",
        "          drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "          if os.path.exists(drive_logs_test_dir): \n",
        "            log_msg(\" Teste existente, pulando: {}\".format(test_name))\n",
        "            continue\n",
        "\n",
        "          drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "          log_msg(\" =========== Executando teste {} ==================\".format(test_name))\n",
        "\n",
        "          model = get_cnn_model(n_inputs, n_outputs, arr_convs_filters_kernels, hidden_layers, batch_norm, dropout, max_pooling, loss=loss_type, timesteps_window=timesteps_window) \n",
        "          model.summary(print_fn=log_msg)\n",
        "\n",
        "          log_msg( \"Input  train size/window/variables = {}/{}/{}\".format(arr_input_train.shape[0], arr_input_train.shape[1], arr_input_train.shape[2]))\n",
        "          log_msg( \"Output train size/window*variables = {}/{}\".format(arr_output_train.shape[0], arr_output_train.shape[1]))\n",
        "\n",
        "          model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "          # Train model\n",
        "          log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "          # ********* agilizando ...\n",
        "          use_gpu()\n",
        "          with tf.device('/device:GPU:0') as gpu:\n",
        "            train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=n_splits)\n",
        "\n",
        "          save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "          # ********* agilizando ...\n",
        "          # use trained model\n",
        "          # model = load_model((model_dir + '/tmp_model.h5'))\n",
        "          validate_model_cnn_timesteps(model, arr_input_val, arr_output_val)\n",
        "          dic_var_default_values = { \\\n",
        "            'qc': 0.0,\n",
        "            'qr': 0.0,\n",
        "            'qi': 0.0,\n",
        "            'qs': 0.0,\n",
        "            'ni': 0.0,\n",
        "            'ns': 0.0,\n",
        "            'nr': 0.0,\n",
        "            'NG': 0.0,\n",
        "            'NC': 0.0,\n",
        "            'EFFCS': 0.891634,\n",
        "            'EFFIS': 0.123289\t\n",
        "          }\n",
        "          # calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values, dic_var_default_values, is_CNN=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  all minmax values: \n",
            "{'k': [1, 28], 'si': [0.0036691060265900003, 1.0], 'Tc': [63.503047882, 308.96236853799996], 'qv': [2.92705568e-06, 0.0162761402701], 'qc': [0.0, 0.000407646693245], 'qr': [0.0, 0.0008443108657339999], 'qi': [0.0, 0.000148722695016], 'qs': [0.0, 0.0016289144075799999], 'qg': [0.0, 0.00038817307111400006], 'ni': [0.0, 1145094.4354], 'ns': [0.0, 2942266.54164], 'nr': [0.0, 1521556.56989], 'NG': [0.0, 6038.50500981], 'NC': [0.0, 35494575.1176], 'tke': [0.0, 6.0], 'kzh': [0.03, 300.0], 'omega': [-1.0769567015000001, 0.5418370767099999], 'EFFCS': [1.0, 27.5819925509], 'EFFIS': [13.0, 130.0], 'LSRAIN': [-1.0830010430299997e-24, 0.00016314062727399998], 'LSSNOW': [-1.0475281590399999e-28, 9.048727267059999e-11]}\n",
            "train size = 4378752\n",
            "Index(['k', 'si', 'Tc', 'qv', 'qc', 'qr', 'qi', 'qs', 'qg', 'ni', 'ns', 'nr',\n",
            "       'NG', 'NC', 'tke', 'kzh', 'omega'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STcsqcJp3pt2"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_Qk_fz30CF"
      },
      "source": [
        "def generate_skill_graphic(skill, skill_fname, key, arr_test, arr_markers, arr_colors, plot_sci_numbers=True):\n",
        "  for test, mark, color in zip(arr_test, arr_markers, arr_colors):\n",
        "    path = '{}/{}/{}'.format(exp1_dir, test, skill_fname)  \n",
        "    df_skill = pd.read_csv(path)\n",
        "    skill_values = df_skill[key].values\n",
        "    plt.plot(skill_values, range(1, len(skill_values)+1), label=test + ' ' + skill, marker=mark, color=color)\n",
        "  if plot_sci_numbers:\n",
        "    plt.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "def generate_subplot(ncols, nskills, idx_plot, key):\n",
        "  plt.subplot(ncols, nskills, idx_plot)\n",
        "  plt.ylabel(\"k\")\n",
        "  plt.xlabel(key)\n",
        "\n",
        "\n",
        "def generate_all_graphics(arr_test, cols, obs_file_name, pred_file_name, arr_skills, arr_skill_filenames, arr_markers, arr_colors, plot_sci_numbers=True):\n",
        "  idx_plot = 1\n",
        "  plt.figure(figsize=(20, 120))\n",
        "  for key in cols:\n",
        "      if key == 'Unnamed: 0': \n",
        "        continue\n",
        "      generate_subplot(len(cols)-1, len(arr_skills)+1, idx_plot, key)\n",
        "\n",
        "      mean_obs_path = '{}/{}/{}'.format(exp1_dir, arr_test[0], obs_file_name)\n",
        "      df_mean_obs = pd.read_csv(mean_obs_path)\n",
        "      obs_values = df_mean_obs[key].values\n",
        "      plt.plot(obs_values, range(1, len(obs_values)+1), label='mean obs',  marker='o', linestyle='dashed', color='yellow', markersize=12)\n",
        "      \n",
        "      generate_skill_graphic('mean pred', pred_file_name, key, arr_test, arr_markers, arr_colors, plot_sci_numbers)\n",
        "      idx_plot += 1\n",
        "\n",
        "      for skill, skill_fname in zip(arr_skills, arr_skill_filenames):\n",
        "        generate_subplot(len(cols)-1, len(arr_skills)+1, idx_plot, key)\n",
        "        generate_skill_graphic(skill, skill_fname, key, arr_test, arr_markers, arr_colors, plot_sci_numbers)\n",
        "        idx_plot += 1\n",
        "  \n",
        "  plt.subplots_adjust(hspace=0.2, wspace=0.1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# exp1_dir = '/content/drive/My Drive/Colab Notebooks/logs_models_rttmg_dt60'\n",
        "exp1_dir = '/content/drive/My Drive/Colab Notebooks/logs'\n",
        "exp1_dir = tb_logdir_base\n",
        "\n",
        "arr_test = [ 'teste_C.6', 'teste_C.6.4_no_spinup', 'teste_C.6.5' ]\n",
        "arr_markers = ['D', 'X', 'P', 's']\n",
        "arr_colors = ['blue','green', 'violet', 'red']\n",
        "\n",
        "# arr_test = [ 'teste_C.6']\n",
        "# arr_markers = ['D']\n",
        "# arr_colors = ['blue']\n",
        "\n",
        "\n",
        "# arr_markers = ['D', 'X', 'P', 's']\n",
        "# arr_colors = ['blue', 'green', 'red', 'violet']\n",
        "dummy_path = '{}/{}/{}'.format(exp1_dir, 'teste_C.6.4', 'log_mean_obs.csv')\n",
        "dummy_df = pd.read_csv(dummy_path)\n",
        "\n",
        "arr_skills = ['bias', 'mse', 'mae']\n",
        "\n",
        "obs_file_name = 'log_mean_obs.csv'\n",
        "pred_file_name = 'log_mean_pred.csv'\n",
        "arr_skill_filenames = ['log_bias.csv', 'log_mse.csv', 'log_mae.csv']\n",
        "generate_all_graphics(arr_test, dummy_df.columns.to_list(), obs_file_name, pred_file_name, arr_skills, arr_skill_filenames, arr_markers, arr_colors)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHyB9r_RAR-h"
      },
      "source": [
        "arr_test = [ 'teste_D.6']\n",
        "arr_markers = ['D']\n",
        "arr_colors = ['blue']\n",
        "generate_all_graphics(arr_test, dummy_df.columns.to_list(), obs_file_name, pred_file_name, arr_skills, arr_skill_filenames, arr_markers, arr_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6WUWhZ0MWaq"
      },
      "source": [
        "#Testes anteriores#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InljpThFZSoW"
      },
      "source": [
        "### Teste C.1\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 18 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQenS_Dvmrn"
      },
      "source": [
        "test_name = \"teste_C.1\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [18]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irZ9YgwIDNvS"
      },
      "source": [
        "#### Teste C.1.1\n",
        "- Idem ao C.1, mas removendo normalização para cálculo do rmse\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 18 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdLrDy4ZDNvl"
      },
      "source": [
        "test_name = \"teste_C.1.1\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = all_examples\n",
        "normalized_all_targets = all_targets\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [18]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ujVdPl6iFQ"
      },
      "source": [
        "#### Teste C.1.2\n",
        "- Idem C.1 mas removendo alguns niveis e colunas, como em B.1.2\n",
        "- camada de entrada com 396 inputs\n",
        "- 1 camada oculta com 18 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyX74lU16iFQ"
      },
      "source": [
        "test_name = \"teste_C.1.2\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  'qc':range(16,29),\n",
        "  'qr':range(16,29),\n",
        "  'qg':range(16,29),\n",
        "  'nc':range(16,29),\n",
        "  'nr':range(16,29),\n",
        "  'ng':range(16,29),\n",
        "  # 'sl':range(1,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [18]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKPiYVhxyvmB"
      },
      "source": [
        "### Teste C.2\n",
        "- Idem C.1 mas com 54 neurônios\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 54 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNAucQIOyvmB"
      },
      "source": [
        "test_name = \"teste_C.2\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [54]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcZJXZbcsfAD"
      },
      "source": [
        "#### Teste C.2.2\n",
        "- Idem C.2 mas removendo alguns niveis e colunas, como em B.1.2\n",
        "- camada de entrada com (#TODO) inputs\n",
        "- 1 camada oculta com 18 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEaE1T69sfAN"
      },
      "source": [
        "test_name = \"teste_C.2.2\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  'qc':range(16,29),\n",
        "  'qr':range(16,29),\n",
        "  'qg':range(16,29),\n",
        "  'nc':range(16,29),\n",
        "  'nr':range(16,29),\n",
        "  'ng':range(16,29),\n",
        "  # 'sl':range(1,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [54]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PZM26V72IoG"
      },
      "source": [
        "### Teste C.3\n",
        "- Idem C.1 mas com 2 camadas com 17 neurônios\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 2 camadas ocultas com 18 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC3JmIqs2IoH"
      },
      "source": [
        "test_name = \"teste_C.3\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [18, 18]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbsKtz_D_-er"
      },
      "source": [
        "### Teste C.4\n",
        "- Idem C.1 mas com com 108 neurônios\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 108 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LQyIt3G_-er"
      },
      "source": [
        "test_name = \"teste_C.4\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [108]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjDWdlk36F_p"
      },
      "source": [
        "#### Teste C.4.2\n",
        "- Idem C.4 mas removendo alguns niveis e colunas, como em B.1.2\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 108 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSOh_3Ne6F_p"
      },
      "source": [
        "test_name = \"teste_C.4.2\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  'qc':range(16,29),\n",
        "  'qr':range(16,29),\n",
        "  'qg':range(16,29),\n",
        "  'nc':range(16,29),\n",
        "  'nr':range(16,29),\n",
        "  'ng':range(16,29),\n",
        "  # 'sl':range(1,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [108]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZC8vN0RkwmY"
      },
      "source": [
        "### Teste C.5\n",
        "- Idem C.1 mas com com 162 neurônios\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 162 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5BveDZJkwmY"
      },
      "source": [
        "test_name = \"teste_C.5\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [162]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Zk1GWwk-GB"
      },
      "source": [
        "### Teste C.6\n",
        "- Idem C.1 mas com com 216 neurônios\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 216 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis -27 níveis LSRAIN LSSONW =  394 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wgwiB45k-GB"
      },
      "source": [
        "test_name = \"teste_C.6\"\n",
        "levels = k_max\n",
        "delta_t = 60\n",
        "spin_hours = 24\n",
        "\n",
        "all_spin_examples, all_spin_targets, no_spinup_size = get_no_spinup_examples_and_targets(levels, delta_t, spin_hours, pre_process=True)\n",
        "    \n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "get_data_for_test(all_spin_examples, all_spin_targets)\n",
        "\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "# Train model\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "# use trained model\n",
        "# model = load_model((model_dir + '/tmp_model.h5'))\n",
        "\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "dic_var_default_values = { \\\n",
        "  'qc': -1.0,\n",
        "  'qr': -1.0,\n",
        "  'qi': -1.0,\n",
        "  'qs': -1.0,\n",
        "  'ni': -1.0,\n",
        "  'ns': -1.0,\n",
        "  'nr': -1.0,\n",
        "  'NG': -1.0,\n",
        "  'NC': -1.0,\n",
        "  'EFFCS': 0.64,\n",
        "  'EFFIS': -0.8\n",
        "}\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values, dic_var_default_values)\n",
        "\n",
        "\n",
        "# FKB Testing ...\n",
        "#\n",
        "print(\"colunas de entrada\")\n",
        "print(normalized_validation_examples.columns.values)\n",
        "print(\"colunas de saída:\")\n",
        "print(normalized_validation_targets.columns.values)\n",
        "\n",
        "arr_one_input_val = normalized_validation_examples.tail(1).to_numpy()\n",
        "arr_one_output_val = normalized_validation_targets.tail(1).to_numpy()\n",
        "print(\"\\n\\n FKB Testing\")\n",
        "print(\"input = \")\n",
        "print(*arr_one_input_val[0], sep=\", \")\n",
        "print(\"output = \")\n",
        "print(*arr_one_output_val[0], sep=\", \")\n",
        "           \n",
        "\n",
        "y = model.predict(arr_one_input_val, use_multiprocessing=True)\n",
        "print(\"prediction h5 = \")                  \n",
        "print(y, sep=\", \")\n",
        "                  \n",
        "\n",
        "from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
        "\n",
        "h5_to_txt(\n",
        "    weights_file_name=model_dir + '/tmp_model.h5', \n",
        "    output_file_name=model_dir + '/fkb_model.txt'\n",
        ")\n",
        "copyfile(model_dir + '/fkb_model.txt', './fkb_model.txt')\n",
        "\n",
        "print(\"\\n\\nprediction FORTRAN = \\n \")                  \n",
        "command_fkb = \"../FKB/build/bin/test_keras ./fkb_model.txt\"\n",
        "!{command_fkb}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qGT6zNg3xX"
      },
      "source": [
        "#### Teste C.6.2 - dt 360\n",
        "- Idem C.6 mas removendo niveis da entrada e da saída\n",
        "- camada de entrada com 396 inputs\n",
        "- 1 camada oculta com 216 neurônios\n",
        "- camada de saída com 396 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7O05VQzg3x1"
      },
      "source": [
        "test_name = \"teste_C.6.2\"\n",
        "\n",
        "get_data_for_test()\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "print('Scale examples:', scale_examples)\n",
        "print('Scale targets:', scale_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  'qc':range(16,29),\n",
        "  'qr':range(16,29),\n",
        "  'qg':range(16,29),\n",
        "  'nc':range(16,29),\n",
        "  'nr':range(16,29),\n",
        "  'ng':range(16,29),\n",
        "  # 'sl':range(1,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ved_HB6rsfQw"
      },
      "source": [
        "#### Teste C.6.3 - dt 60\n",
        "\n",
        "- Idem C.6 mas removendo niveis da entrada e da saída\n",
        "- camada de entrada com 217 inputs\n",
        "- 1 camada oculta com 216 neurônios\n",
        "- camada de saída com 227 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4U40aVQsfQ1"
      },
      "source": [
        "test_name = \"teste_C.6.3\"\n",
        "\n",
        "get_data_for_test()\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  # 'sl': range(1,29),                          \n",
        "  'qv': range(20,29),                          \n",
        "  'qc': [3, 4] + list(range(13,29)),\n",
        "  'qr': range(13,29),\n",
        "  'qi': list(range(1,25)) + list(range(26,29)),\n",
        "  'qs': list(range(1,11)) + [27, k_max],\n",
        "# TODO - checar porquê qg apresenta 0 e 1 na entrada, mas somente 0 na saída  \n",
        "  'qg': list(range(1,11)) + list(range(16,29)),\n",
        "\n",
        "  'ni': list(range(1,25)) + list(range(26,29)),\n",
        "  'ns': list(range(1,15)) + [27, k_max],\n",
        "  'nr': range(13,29),\n",
        "  'NG': list(range(1,10)) + [22, 23, 27, k_max],\n",
        "  'NC': [3, 4] + list(range(13,29)),\n",
        "\n",
        "  'tke': list(range(9,24)) + [25, 26],\n",
        "  'kzh': range(10,23),\n",
        "  'omega': range(23,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  'qv': range(20,29),                          \n",
        "  'qc': [3, 4] + list(range(13,29)),\n",
        "  'qr': range(13,29),\n",
        "  'qi': list(range(1,25)) + list(range(26,29)),\n",
        "  'qs': list(range(1,11)) + [27, k_max],\n",
        "# TODO - checar porquê qg apresenta 0 e 1 na entrada, mas somente 0 na saída  \n",
        "  'qg': list(range(1,10)) + [22, 23, 27, k_max],\n",
        "\n",
        "  'ni': list(range(1,25)) + list(range(26,29)),\n",
        "  'ns': list(range(1,15)) + [27, k_max],\n",
        "  'nr': range(13,29),\n",
        "  'NG': list(range(1,10)) + [22, 23, 27, k_max],\n",
        "  'NC': [3, 4] + list(range(13,29)),\n",
        "\n",
        "  'EFFCS': [3, 4]  + list(range(13,29)),\n",
        "  'EFFIS': list(range(1,12)) + list(range(20,24)) + list(range(26,29))\n",
        "}\n",
        "\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "\n",
        "\n",
        "# train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "# save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "rmtree('./tmp_model', ignore_errors=True)\n",
        "copytree(model_dir, './tmp_model')\n",
        "model = load_model('./tmp_model')\n",
        "\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRyz7mq1ZbMh"
      },
      "source": [
        "#### Teste C.6.4 - dt 60 - no spinup\n",
        "\n",
        "- Evolução do C.6.4 alterando níveis e sem spinup\n",
        "- camada de entrada com TODO inputs\n",
        "- 1 camada oculta com 216 neurônios\n",
        "- camada de saída com TODO outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elq43rf9ZbNL"
      },
      "source": [
        "test_name = \"teste_C.6.4_no_spinup\"\n",
        "levels = k_max\n",
        "delta_t = 60\n",
        "spin_hours = 24\n",
        "\n",
        "all_spin_examples, all_spin_targets, no_spinup_size = get_no_spinup_examples_and_targets(levels, delta_t, spin_hours, pre_process=True)\n",
        "    \n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "get_data_for_test(all_spin_examples, all_spin_targets)\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  # 'sl': range(1,29),                          \n",
        "  'qv': range(20,29),\n",
        "  'qc': [3, 4] + list(range(13,29)),\n",
        "  'qr': range(13,29),\n",
        "  'qi': list(range(1,25)) + list(range(26,29)),\n",
        "  'qs': list(range(1,11)) + [27, k_max],\n",
        "# TODO - checar porquê qg apresenta 0 e 1 na entrada, mas somente 0 na saída  \n",
        "  # 'qg': list(range(1,11)) + list(range(16,29)),\n",
        "#   'ni': list(range(1,12)) + list(range(20,25)) + list(range(26,29)),\n",
        "  'ni': list(range(1,25)) + list(range(26,29)),                          \n",
        "  'ns': list(range(1,16)) + [27, k_max],\n",
        "  'nr': range(13,29),\n",
        "  'NG': list(range(1,9)) + [22, 23, 27, k_max],\n",
        "  'NC': [3, 4] + list(range(13,29)),\n",
        "  'tke': [1] + list(range(9,24)) + [25, 26],\n",
        "  'kzh': range(10,23),\n",
        "  'omega': range(23,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  'qc': [3, 4] + list(range(13,29)),\n",
        "  'qr': range(13,29),\n",
        "  'qi': list(range(1,25)) + list(range(26,29)),\n",
        "  'qs': list(range(1,11)) + [27, k_max],\n",
        "# TODO - checar porquê qg apresenta 0 e 1 na entrada, mas somente 0 na saída  \n",
        "  # 'qg': list(range(1,10)) + [22, 23, 27, k_max],\n",
        "#   'ni': list(range(1,12)) + list(range(20,25)) + list(range(26,29)),\n",
        "  'ni': list(range(1,25)) + list(range(26,29)),                          \n",
        "  'ns': list(range(1,16)) + [27, k_max],\n",
        "  'nr': range(13,29),\n",
        "  'NG': list(range(1,9)) + [22, 23, 27, k_max],\n",
        "  'NC': [3, 4] + list(range(13,29)),\n",
        "  'EFFCS': list(range(1,8)) + list(range(9,29)),\n",
        "  'EFFIS': list(range(1,29))\n",
        "}\n",
        "\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=50\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "# Train model\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=1)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "# use trained model\n",
        "# model = load_model((model_dir + '/tmp_model.h5'))\n",
        "\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "dic_var_default_values = { \\\n",
        "  'qc': -1.0,\n",
        "  'qr': -1.0,\n",
        "  'qi': -1.0,\n",
        "  'qs': -1.0,\n",
        "  'ni': -1.0,\n",
        "  'ns': -1.0,\n",
        "  'nr': -1.0,\n",
        "  'NG': -1.0,\n",
        "  'NC': -1.0,\n",
        "  'EFFCS': 0.64,\n",
        "  'EFFIS': -0.8\n",
        "}\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values, dic_var_default_values)\n",
        "\n",
        "# FKB Testing ...\n",
        "#\n",
        "print(\"colunas de entrada\")\n",
        "print(normalized_validation_examples.columns.values)\n",
        "print(\"colunas de saída:\")\n",
        "print(normalized_validation_targets.columns.values)\n",
        "\n",
        "arr_one_input_val = normalized_validation_examples.tail(1).to_numpy()\n",
        "arr_one_output_val = normalized_validation_targets.tail(1).to_numpy()\n",
        "print(\"\\n\\n FKB Testing\")\n",
        "print(\"input = \")\n",
        "print(*arr_one_input_val[0], sep=\", \")\n",
        "print(\"output = \")\n",
        "print(*arr_one_output_val[0], sep=\", \")\n",
        "           \n",
        "\n",
        "y = model.predict(arr_one_input_val, use_multiprocessing=True)\n",
        "print(\"prediction h5 = \")                  \n",
        "print(y, sep=\", \")\n",
        "                  \n",
        "\n",
        "from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
        "\n",
        "h5_to_txt(\n",
        "    weights_file_name=model_dir + '/tmp_model.h5', \n",
        "    output_file_name=model_dir + '/fkb_model.txt'\n",
        ")\n",
        "copyfile(model_dir + '/fkb_model.txt', './fkb_model.txt')\n",
        "\n",
        "# print(\"\\n\\nprediction FORTRAN = \\n \")                  \n",
        "# command_fkb = \"../FKB/build/bin/test_keras ./fkb_model.txt\"\n",
        "# !{command_fkb}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvNfqmShcqX7"
      },
      "source": [
        "#### Teste C.6.5 - dt 60\n",
        "\n",
        "- Idem C.4 mas removendo apenas niveis da entrada (na tentativa da rede tentar decifrar todos os níveis das saídas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svdnydLtcqYV",
        "scrolled": true
      },
      "source": [
        "test_name = \"teste_C.6.5\"\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "get_data_for_test()\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  # 'sl': range(1,29),                          \n",
        "  'qv': range(20,29),\n",
        "  'qc': [3, 4] + list(range(13,29)),\n",
        "  'qr': range(13,29),\n",
        "  'qi': list(range(1,25)) + list(range(26,29)),\n",
        "# TODO - checar porquê qg apresenta 0 e 1 na entrada, mas somente 0 na saída  \n",
        "\n",
        "  # 'qg': list(range(1,11)) + list(range(16,29)),\n",
        "  'ni': list(range(1,12)) + list(range(20,25)) + list(range(26,29)),\n",
        "  'ns': list(range(1,16)) + [27, k_max],\n",
        "  'nr': range(13,29),\n",
        "  'NG': list(range(1,9)) + [22, 23, 27, k_max],\n",
        "  'NC': [3, 4] + list(range(13,29)),\n",
        "  'tke': [1] + list(range(9,24)) + [25, 26],\n",
        "  'kzh': range(10,23),\n",
        "  'omega': range(23,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "# Train model\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "# train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=10)\n",
        "# save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "# use trained model\n",
        "model = load_model((model_dir + '/tmp_model.h5'))\n",
        "\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values)\n",
        "\n",
        "# FKB Testing ...\n",
        "#\n",
        "print(\"colunas de entrada\")\n",
        "print(normalized_validation_examples.columns.values)\n",
        "print(\"colunas de saída:\")\n",
        "print(normalized_validation_targets.columns.values)\n",
        "\n",
        "arr_one_input_val = normalized_validation_examples.tail(1).to_numpy()\n",
        "arr_one_output_val = normalized_validation_targets.tail(1).to_numpy()\n",
        "print(\"\\n\\n FKB Testing\")\n",
        "print(\"input = \")\n",
        "print(*arr_one_input_val[0], sep=\", \")\n",
        "print(\"output = \")\n",
        "print(*arr_one_output_val[0], sep=\", \")\n",
        "           \n",
        "\n",
        "y = model.predict(arr_one_input_val, use_multiprocessing=True)\n",
        "print(\"prediction h5 = \")                  \n",
        "print(y, sep=\", \")\n",
        "                  \n",
        "\n",
        "from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
        "\n",
        "h5_to_txt(\n",
        "    weights_file_name=model_dir + '/tmp_model.h5', \n",
        "    output_file_name=model_dir + '/fkb_model.txt'\n",
        ")\n",
        "copyfile(model_dir + '/fkb_model.txt', './fkb_model.txt')\n",
        "\n",
        "print(\"\\n\\nprediction FORTRAN = \\n \")                  \n",
        "command_fkb = \"../FKB/build/bin/test_keras ./fkb_model.txt\"\n",
        "!{command_fkb}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_aa4eRzvZG0"
      },
      "source": [
        "### Teste C.7\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camadas ocultas [476] - mesma qtdd de neurônios que a camada de entrada\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAGpHV-xvZG0"
      },
      "source": [
        "test_name = \"teste_C.7\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [476]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrq1vqNqsU1y"
      },
      "source": [
        "#### Teste C.7.5 - dt 60\n",
        "\n",
        "- Idem C.6.5 - removendo apenas niveis da entrada (na tentativa da rede tentar decifrar todos os níveis das saídas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqVAfISTsU2P"
      },
      "source": [
        "test_name = \"teste_C.7.5\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "dic_var_levels_exclude = { \\\n",
        "  # 'sl': range(1,29),                          \n",
        "  'qv': range(20,29),\n",
        "  'qc': [3, 4] + list(range(13,29)),\n",
        "  'qr': range(13,29),\n",
        "  'qi': list(range(1,25)) + list(range(26,29)),\n",
        "# TODO - checar porquê qg apresenta 0 e 1 na entrada, mas somente 0 na saída  \n",
        "\n",
        "  # 'qg': list(range(1,11)) + list(range(16,29)),\n",
        "  'ni': list(range(1,12)) + list(range(20,25)) + list(range(26,29)),\n",
        "  'ns': list(range(1,16)) + [27, k_max],\n",
        "  'nr': range(13,29),\n",
        "  'NG': list(range(1,9)) + [22, 23, 27, k_max],\n",
        "  'NC': [3, 4] + list(range(13,29)),\n",
        "  'tke': [1] + list(range(9,24)) + [25, 26],\n",
        "  'kzh': range(10,23),\n",
        "  'omega': range(23,29)\n",
        "}\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final, dic_var_levels_exclude)\n",
        "\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [476]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Ylo2WRlOVv"
      },
      "source": [
        "### Teste C.8\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 2 camadas ocultas [216, 216]\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f-9dC6BlOVw"
      },
      "source": [
        "test_name = \"teste_C.8\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216, 216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtF5POEzvws8"
      },
      "source": [
        "### Teste C.9\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camadas ocultas  [476, 476]\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNG_5-BPvws8"
      },
      "source": [
        "test_name = \"teste_C.9\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [476, 476]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, hidden_layers, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_targets_min_values, all_targets_max_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEdRpPbWwaWv"
      },
      "source": [
        "### Teste C.10\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camadas ocultas [476, 216]\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMJBXdUawaWv"
      },
      "source": [
        "test_name = \"teste_C.10\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [476, 216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs, target_key)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB7y1ECJxDY5"
      },
      "source": [
        "### Teste C.11\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camadas ocultas [476, 448]\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsH4IZ8FxDY5"
      },
      "source": [
        "test_name = \"teste_C.11\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [476, 448]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs, target_key)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygos1XKexXii"
      },
      "source": [
        "### Teste C.12\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- camadas ocultas [k_max, k_max, k_max]\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNFdn0VzxXii"
      },
      "source": [
        "test_name = \"teste_C.12\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [k_max, k_max, k_max]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs, target_key)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t62odgKE7MbX"
      },
      "source": [
        "### Teste C.13\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- camadas ocultas [10, 10, 10]\n",
        "- camada de saída com 16 variáveis x k_max níveis =  448 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOAXWqcL7MbY"
      },
      "source": [
        "test_name = \"teste_C.13\"\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "normalized_all_examples = normalize_linear_scale(all_examples)\n",
        "normalized_all_targets = normalize_linear_scale(all_targets)\n",
        "\n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets = \\\n",
        "get_percent_normalized_trainining_and_validation(normalized_all_examples, normalized_all_targets, 0.9)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [10, 10, 10]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "# Train model\n",
        "#\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_i### Teste C.6\n",
        "- Idem C.1 mas com com 216 neurônios\n",
        "- camada de entrada com 17 variáveis x k_max níveis = 476 inputs\n",
        "- 1 camada oculta com 216 neurônios\n",
        "- camada de saída com 16 variáveis x k_max níveis -27 níveis LSRAIN LSSONW =  394 outnputs\n",
        "- Executar a rede nerual com normalização linear\n",
        "- utilizando esquema de 5 splits e Early stop\n",
        "\n",
        "test_name = \"teste_C.6\"\n",
        "levels = k_max\n",
        "delta_t = 60\n",
        "spin_hours = 24\n",
        "\n",
        "all_spin_examples, all_spin_targets, no_spinup_size = get_no_spinup_examples_and_targets(levels, delta_t, spin_hours, pre_process=True)\n",
        "    \n",
        "normalized_training_examples, normalized_training_targets, normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values = \\\n",
        "get_data_for_test(all_spin_examples, all_spin_targets)\n",
        "\n",
        "\n",
        "drive_logs_test_dir = create_drive_test_log_dir(test_name)\n",
        "# drive_logs_test_dir = get_drive_test_log_dir(test_name)\n",
        "\n",
        "k_inicial = 1\n",
        "k_final = k_max\n",
        "\n",
        "normalized_training_examples = get_df_col_k(normalized_training_examples, k_inicial, k_final)\n",
        "normalized_training_targets = get_df_col_k(normalized_training_targets, k_inicial, k_final)\n",
        "normalized_validation_examples = get_df_col_k(normalized_validation_examples, k_inicial, k_final)\n",
        "normalized_validation_targets = get_df_col_k(normalized_validation_targets, k_inicial, k_final)\n",
        "\n",
        "arr_input_train = normalized_training_examples.to_numpy()\n",
        "arr_output_train = normalized_training_targets.to_numpy()\n",
        "\n",
        "# Train model parameters\n",
        "#\n",
        "loss_type = 'mse'\n",
        "epochs=200\n",
        "n_inputs = arr_input_train.shape[1]\n",
        "hidden_layers = [216]  \n",
        "n_outputs = arr_output_train.shape[1]\n",
        "\n",
        "model = get_custom_model(n_inputs, n_outputs, hidden_layers, loss=loss_type) \n",
        "model.summary(print_fn=log_msg)\n",
        "\n",
        "log_msg( \"Input  train variables/size = {}/{}\".format(n_inputs, arr_input_train.shape[0]))\n",
        "log_msg( \"Output train variables/size = {}/{} \".format(n_outputs, arr_output_train.shape[0]))\n",
        "\n",
        "model_dir = get_mode_var_name_dir(test_name, n_inputs, hidden_layers, n_outputs, epochs, 'ALL_VARS')\n",
        "\n",
        "# Train model\n",
        "log_dir_var = get_tb_logdir_var(test_name, hidden_layers, epochs)\n",
        "train_model(model, arr_input_train, arr_output_train, log_dir_var, epochs=epochs, n_splits=10)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "\n",
        "# use trained model\n",
        "# model = load_model((model_dir + '/tmp_model.h5'))\n",
        "\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)\n",
        "dic_var_default_values = { \\\n",
        "  'qc': -1.0,\n",
        "  'qr': -1.0,\n",
        "  'qi': -1.0,\n",
        "  'qs': -1.0,\n",
        "  'ni': -1.0,\n",
        "  'ns': -1.0,\n",
        "  'nr': -1.0,\n",
        "  'NG': -1.0,\n",
        "  'NC': -1.0,\n",
        "  'EFFCS': 0.64,\n",
        "  'EFFIS': -0.8\n",
        "}\n",
        "calc_skill_C(normalized_validation_examples, normalized_validation_targets, all_min_values, all_max_values, dic_var_default_values)\n",
        "\n",
        "\n",
        "# FKB Testing ...\n",
        "#\n",
        "print(\"colunas de entrada\")\n",
        "print(normalized_validation_examples.columns.values)\n",
        "print(\"colunas de saída:\")\n",
        "print(normalized_validation_targets.columns.values)\n",
        "\n",
        "arr_one_input_val = normalized_validation_examples.tail(1).to_numpy()\n",
        "arr_one_output_val = normalized_validation_targets.tail(1).to_numpy()\n",
        "print(\"\\n\\n FKB Testing\")\n",
        "print(\"input = \")\n",
        "print(*arr_one_input_val[0], sep=\", \")\n",
        "print(\"output = \")\n",
        "print(*arr_one_output_val[0], sep=\", \")\n",
        "           \n",
        "\n",
        "y = model.predict(arr_one_input_val, use_multiprocessing=True)\n",
        "print(\"prediction h5 = \")                  \n",
        "print(y, sep=\", \")\n",
        "                  \n",
        "\n",
        "from KerasWeightsProcessing.convert_weights import txt_to_h5, h5_to_txt\n",
        "\n",
        "h5_to_txt(\n",
        "    weights_file_name=model_dir + '/tmp_model.h5', \n",
        "    output_file_name=model_dir + '/fkb_model.txt'\n",
        ")\n",
        "copyfile(model_dir + '/fkb_model.txt', './fkb_model.txt')\n",
        "\n",
        "print(\"\\n\\nprediction FORTRAN = \\n \")                  \n",
        "command_fkb = \"../FKB/build/bin/test_keras ./fkb_model.txt\"\n",
        "!{command_fkb}nput_train, arr_output_train, log_dir_var, epochs=epochs)\n",
        "save_model_and_logs(model, test_name, n_inputs, n_outputs, hidden_layers, epochs, log_dir_var, drive_logs_test_dir)\n",
        "validate_model(model, normalized_validation_examples, normalized_validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}